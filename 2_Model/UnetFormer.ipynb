{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLoUbbWfnnTQ"
   },
   "source": [
    "# 0. 초기 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-LjVNJocZ2IR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Pytorch에서 gpu를 사용하는 방법.\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import tifffile as tiff\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vcspYaHHKoOq"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-KLMtHmhco7"
   },
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yt_htC4wnOFZ"
   },
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = './pro_data/data_for_training_and_testing/train/images/'\n",
    "train_mask_dir = './pro_data/data_for_training_and_testing/train/masks/'\n",
    "\n",
    "valid_img_dir = './pro_data/data_for_training_and_testing/val/images/'\n",
    "valid_mask_dir = './pro_data/data_for_training_and_testing/val/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_img = sorted(os.listdir(train_img_dir))\n",
    "data_train_mask = sorted(os.listdir(train_mask_dir))\n",
    "\n",
    "data_val_img = sorted(os.listdir(valid_img_dir))\n",
    "data_val_mask = sorted(os.listdir(valid_mask_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'img_path': data_train_img, 'mask_path': data_train_mask})\n",
    "df_valid = pd.DataFrame({'img_path': data_val_img, 'mask_path': data_val_mask})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DXpo9QomS99"
   },
   "source": [
    "`1` Quick understanding of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIWesMLcmfax"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = pd.read_csv('./train.csv'), pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_Kp_n4ghi4R"
   },
   "outputs": [],
   "source": [
    "temp_img = cv2.imread(df_train.loc[0, 'img_path']) #3 channels / spectral bands\n",
    "plt.imshow(temp_img[:,:,2]) #View each channel...\n",
    "temp_mask = rle_decode(df_train.loc[0, 'mask_rle'], shape = (1024, 1024)) #1 channels\n",
    "labels, count = np.unique(temp_mask[:, :], return_counts=True) #Check for each channel. All chanels are identical\n",
    "print(\"Labels are: \", labels, \" and the counts are: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_ebfl6anlx9"
   },
   "source": [
    "`2` Now, crop each large image into patches of 224x224. Save them into a directory\n",
    "\n",
    "so we can use data augmentation and read directly from the drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHd5J5qon4zY"
   },
   "outputs": [],
   "source": [
    "patch_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LOiBsh8sHBq"
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_train)):\n",
    "    img_path = df_train.loc[i, 'img_path']\n",
    "    image = cv2.imread(img_path)       #Read each image as BGR\n",
    "    SIZE_X = (image.shape[1] // patch_size) * patch_size  #Nearest size divisible by our patch size\n",
    "    SIZE_Y = (image.shape[0] // patch_size) * patch_size  #Nearest size divisible by our patch size\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.crop((0 ,0, SIZE_X, SIZE_Y))\n",
    "    image = np.array(image)\n",
    "\n",
    "    #Extract patches from each image\n",
    "    print(\"Now patchifying image:\", img_path)\n",
    "    patches_img = patchify(image, (224, 224, 3), step=224)\n",
    "\n",
    "    for j in range(patches_img.shape[0]):\n",
    "        for k in range(patches_img.shape[1]):\n",
    "\n",
    "            single_patch_img = patches_img[j,k,:,:]\n",
    "            single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.\n",
    "\n",
    "            cv2.imwrite(\"224_patches/images/\"+ df_train.loc[i, 'img_id'] + \"patch_\"+str(j)+str(k)+\".png\", single_patch_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y45hH2nioDT7"
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_train)):\n",
    "    mask = rle_decode(df_train.loc[i, 'mask_rle'], shape = (1024, 1024))\n",
    "    SIZE_X = (mask.shape[1] // patch_size) * patch_size  #Nearest size divisible by our patch size\n",
    "    SIZE_Y = (mask.shape[0] // patch_size) * patch_size  #Nearest size divisible by our patch size\n",
    "    mask = Image.fromarray(mask)\n",
    "    mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    #Extract patches from each image\n",
    "    print(\"Now patchifying mask:\", i)\n",
    "    patches_mask = patchify(mask, (224, 224), step=224)\n",
    "\n",
    "    for j in range(patches_mask.shape[0]):\n",
    "        for k in range(patches_mask.shape[1]):\n",
    "\n",
    "            single_patch_mask = patches_mask[j,k,:,:]\n",
    "\n",
    "            cv2.imwrite(\"./224_patches/masks/\"+ \"MASK_\" + df_train.loc[i, 'img_id'][-4:] + \"patch_\"+str(j)+str(k)+\".png\", single_patch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GiIXUMBX4En"
   },
   "outputs": [],
   "source": [
    "image_test = cv2.imread(\"./224_patches/images/TRAIN_0000patch_01.png\", 1)\n",
    "image_test = cv2.cvtColor(image_test, cv2.COLOR_BGR2RGB)\n",
    "mask_test = cv2.imread(\"./224_patches/masks/MASK_0000patch_01.png\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkTFK0OHW5lX"
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoZKkMWOoDMS"
   },
   "outputs": [],
   "source": [
    "plt.imshow(mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STHbOn1wQNYa"
   },
   "outputs": [],
   "source": [
    "train_img_dir = \"224_patches/images/\"\n",
    "train_mask_dir = \"224_patches/masks/\"\n",
    "\n",
    "img_list = os.listdir(train_img_dir)\n",
    "msk_list = os.listdir(train_mask_dir)\n",
    "\n",
    "num_images = len(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBZ-OO4Gc2O-"
   },
   "outputs": [],
   "source": [
    "print(len(img_list), len(msk_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDiey88moDJr"
   },
   "outputs": [],
   "source": [
    "img_num = random.randint(0, num_images-1)\n",
    "\n",
    "img_for_plot = cv2.imread(train_img_dir + img_list[img_num], 1)\n",
    "img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask_for_plot = cv2.imread(train_mask_dir + msk_list[img_num], 0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_for_plot)\n",
    "plt.title('Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_for_plot, cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iulfslKoDDN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YccnKcjToC8x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agIhT0OloC1C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyrboHN8hjEW"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsScGmliaF8G"
   },
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4Q1208saGRL"
   },
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWNuQiT4xQ5u"
   },
   "outputs": [],
   "source": [
    "class SatelliteDatasetForValid(Dataset):\n",
    "    def __init__(self, dataset, transform=None, infer=False):\n",
    "        self.data = dataset\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m80Qh2tL9_ZK"
   },
   "outputs": [],
   "source": [
    "class SatelliteDatasetForTest(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = sr.upsample(image)\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = sr.upsample(image)\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "494dbxsJaAD5"
   },
   "source": [
    "# 1. 데이터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agkeQt9Dn5VJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UVbCb4bZ0KP"
   },
   "outputs": [],
   "source": [
    "# # Data Loader\n",
    "transform = A.Compose([\n",
    "    # A.Resize(224, 224),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "data = pd.read_csv(\"./train.csv\")\n",
    "train, valid = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "trainset = SatelliteDatasetForValid(dataset = train, transform=transform)\n",
    "validset = SatelliteDatasetForValid(dataset = valid, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=8, shuffle=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(validset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN9Xy-KOuax7"
   },
   "source": [
    "#### 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rdoiwes2vbhU"
   },
   "outputs": [],
   "source": [
    "img_path = train['img_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwfj_3zavYzm"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zG6vYF7vdTw"
   },
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wBLQlvdvyxw"
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "\n",
    "def view_as_windows(arr_in, window_shape, step=1):\n",
    "\n",
    "    # -- basic checks on arguments\n",
    "    if not isinstance(arr_in, np.ndarray):\n",
    "        raise TypeError(\"`arr_in` must be a numpy ndarray\")\n",
    "\n",
    "    ndim = arr_in.ndim\n",
    "\n",
    "    if isinstance(window_shape, numbers.Number):\n",
    "        window_shape = (window_shape,) * ndim\n",
    "    if not (len(window_shape) == ndim):\n",
    "        raise ValueError(\"`window_shape` is incompatible with `arr_in.shape`\")\n",
    "\n",
    "    if isinstance(step, numbers.Number):\n",
    "        if step < 1:\n",
    "            raise ValueError(\"`step` must be >= 1\")\n",
    "        step = (step,) * ndim\n",
    "    if len(step) != ndim:\n",
    "        raise ValueError(\"`step` is incompatible with `arr_in.shape`\")\n",
    "\n",
    "    arr_shape = np.array(arr_in.shape)\n",
    "    window_shape = np.array(window_shape, dtype=arr_shape.dtype)\n",
    "\n",
    "    if ((arr_shape - window_shape) < 0).any():\n",
    "        raise ValueError(\"`window_shape` is too large\")\n",
    "\n",
    "    if ((window_shape - 1) < 0).any():\n",
    "        raise ValueError(\"`window_shape` is too small\")\n",
    "\n",
    "    # -- build rolling window view\n",
    "    slices = tuple(slice(None, None, st) for st in step)\n",
    "    window_strides = np.array(arr_in.strides)\n",
    "\n",
    "    indexing_strides = arr_in[slices].strides\n",
    "\n",
    "    win_indices_shape = (\n",
    "        (np.array(arr_in.shape) - np.array(window_shape)) // np.array(step)\n",
    "    ) + 1\n",
    "\n",
    "    new_shape = tuple(list(win_indices_shape) + list(window_shape))\n",
    "    strides = tuple(list(indexing_strides) + list(window_strides))\n",
    "    print(strides)\n",
    "\n",
    "    arr_out = as_strided(arr_in, shape=new_shape, strides=strides)\n",
    "    return arr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ulkp9zznvuZn"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, cast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "Imsize = Union[Tuple[int, int], Tuple[int, int, int]]\n",
    "\n",
    "\n",
    "def patchify(image: np.ndarray, patch_size: Imsize, step: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Split a 2D or 3D image into small patches given the patch size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: the image to be split. It can be 2d (m, n) or 3d (k, m, n)\n",
    "    patch_size: the size of a single patch\n",
    "    step: the step size between patches\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> image = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "    >>> patches = patchify(image, (2, 2), step=1)  # split image into 2*3 small 2*2 patches.\n",
    "    >>> assert patches.shape == (2, 3, 2, 2)\n",
    "    >>> reconstructed_image = unpatchify(patches, image.shape)\n",
    "    >>> assert (reconstructed_image == image).all()\n",
    "    \"\"\"\n",
    "    return view_as_windows(image, patch_size, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2-_lQiOvm8d"
   },
   "outputs": [],
   "source": [
    "patches = patchify(image, (224, 224, 3), step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_H3NB0juyIqX"
   },
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQ5mMWufyL_x"
   },
   "outputs": [],
   "source": [
    "(patches[0][1][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8bWOzw5v97C"
   },
   "outputs": [],
   "source": [
    "(patches[0][0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAg6LTmCztIo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXijWn--4kCX"
   },
   "outputs": [],
   "source": [
    "data224 = [patches[i][j][0] for i in range(9) for j in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upgTKDcL5l3v"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (100, 100))\n",
    "rows = 27\n",
    "cols = 3\n",
    "\n",
    "for i in range(81):\n",
    "    ax = fig.add_subplot(rows, cols, i+1)\n",
    "    ax.imshow(cv2.cvtColor(data224[i], cv2.COLOR_BGR2RGB))\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-qTXN9ExdzL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(patches[8][2][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFsT76OH7PZp"
   },
   "outputs": [],
   "source": [
    "mask0 = rle_decode(train['mask_rle'][0], shape = (1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEDERmCC873S"
   },
   "outputs": [],
   "source": [
    "patches_mask = patchify(mask0, (224, 224), step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zfp5lS3-9BkM"
   },
   "outputs": [],
   "source": [
    "patches_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcHjE_8b9GT4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(patches_mask[8][2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1MHTlzwoCHC"
   },
   "source": [
    "# 2. 데이터 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GISzDeAooxmy"
   },
   "source": [
    "### 0) import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpy9MQ8eK8Lf"
   },
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcYL0fzeluq1"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-yzZzBelqQd"
   },
   "source": [
    "### 1) Frist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q95lc69dK9a1"
   },
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "\n",
    "\n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n",
    "        super(ConvBN, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n",
    "            norm_layer(out_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "class Conv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, bias=False):\n",
    "        super(Conv, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n",
    "                 norm_layer=nn.BatchNorm2d):\n",
    "        super(SeparableConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            norm_layer(out_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConvBN(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n",
    "                 norm_layer=nn.BatchNorm2d):\n",
    "        super(SeparableConvBN, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            norm_layer(out_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1):\n",
    "        super(SeparableConv, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU6, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1, 1, 0, bias=True)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1, 1, 0, bias=True)\n",
    "        self.drop = nn.Dropout(drop, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.permute(0,3,1,2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalLocalAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim=256,\n",
    "                 num_heads=16,\n",
    "                 qkv_bias=False,\n",
    "                 window_size=8,\n",
    "                 relative_pos_embedding=True\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.ws = window_size\n",
    "\n",
    "        self.qkv = Conv(dim, 3*dim, kernel_size=1, bias=qkv_bias)\n",
    "        self.local1 = ConvBN(dim, dim, kernel_size=3)\n",
    "        self.local2 = ConvBN(dim, dim, kernel_size=1)\n",
    "        self.proj = SeparableConvBN(dim, dim, kernel_size=window_size)\n",
    "\n",
    "        self.attn_x = nn.AvgPool2d(kernel_size=(window_size, 1), stride=1,  padding=(window_size//2 - 1, 0))\n",
    "        self.attn_y = nn.AvgPool2d(kernel_size=(1, window_size), stride=1, padding=(0, window_size//2 - 1))\n",
    "\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            # define a parameter table of relative position bias\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(self.ws)\n",
    "            coords_w = torch.arange(self.ws)\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += self.ws - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.ws - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.ws - 1\n",
    "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "            trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "\n",
    "    def pad(self, x, ps):\n",
    "        _, _, H, W = x.size()\n",
    "        # print(f\"W: {W}, H: {H}, ps: {ps}, W % ps: {W % ps}, H % ps: {H % ps}\")\n",
    "        if W % ps != 0:\n",
    "            x = F.pad(x, (0, ps - W % ps), mode='reflect')\n",
    "        if H % ps != 0:\n",
    "            x = F.pad(x, (0, 0, 0, ps - H % ps), mode='reflect')\n",
    "        return x\n",
    "\n",
    "    def pad_out(self, x):\n",
    "        x = F.pad(x, pad=(0, 1, 0, 1), mode='reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.permute(0,3,1,2)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        local = self.local2(x) + self.local1(x)\n",
    "\n",
    "        x = self.pad(x, self.ws)\n",
    "        B, C, Hp, Wp = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        q, k, v = rearrange(qkv, 'b (qkv h d) (hh ws1) (ww ws2) -> qkv (b hh ww) h (ws1 ws2) d', h=self.num_heads,\n",
    "                            d=C//self.num_heads, hh=Hp//self.ws, ww=Wp//self.ws, qkv=3, ws1=self.ws, ws2=self.ws)\n",
    "\n",
    "        dots = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.ws * self.ws, self.ws * self.ws, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            dots += relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        attn = attn @ v\n",
    "\n",
    "        attn = rearrange(attn, '(b hh ww) h (ws1 ws2) d -> b (h d) (hh ws1) (ww ws2)', h=self.num_heads,\n",
    "                         d=C//self.num_heads, hh=Hp//self.ws, ww=Wp//self.ws, ws1=self.ws, ws2=self.ws)\n",
    "\n",
    "\n",
    "        attn = attn[:, :, :H, :W]\n",
    "\n",
    "        out = self.attn_x(F.pad(attn, pad=(0, 0, 0, 1), mode='reflect')) + \\\n",
    "              self.attn_y(F.pad(attn, pad=(0, 1, 0, 0), mode='reflect'))\n",
    "\n",
    "        out = out + local\n",
    "        out = self.pad_out(out)\n",
    "        out = self.proj(out)\n",
    "        # print(out.size())\n",
    "        out = out[:, :, :H, :W]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim=256, num_heads=16,  mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.ReLU6, norm_layer=nn.BatchNorm2d, window_size=8):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = GlobalLocalAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, window_size=window_size)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, act_layer=act_layer, drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.permute(0,3,1,2)\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class WF(nn.Module):\n",
    "    def __init__(self, in_channels=128, decode_channels=128, eps=1e-8):\n",
    "        super(WF, self).__init__()\n",
    "        self.pre_conv = Conv(in_channels, decode_channels, kernel_size=1)\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n",
    "        self.eps = eps\n",
    "        self.post_conv = ConvBNReLU(decode_channels, decode_channels, kernel_size=3)\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        # x = x.permute(0,3,1,2)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        weights = nn.ReLU()(self.weights)\n",
    "        fuse_weights = weights / (torch.sum(weights, dim=0) + self.eps)\n",
    "        x = fuse_weights[0] * self.pre_conv(res) + fuse_weights[1] * x\n",
    "        x = self.post_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatureRefinementHead(nn.Module):\n",
    "    def __init__(self, in_channels=64, decode_channels=64):\n",
    "        super().__init__()\n",
    "        self.pre_conv = Conv(in_channels, decode_channels, kernel_size=1)\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n",
    "        self.eps = 1e-8\n",
    "        self.post_conv = ConvBNReLU(decode_channels, decode_channels, kernel_size=3)\n",
    "\n",
    "        self.pa = nn.Sequential(nn.Conv2d(decode_channels, decode_channels, kernel_size=3, padding=1, groups=decode_channels),\n",
    "                                nn.Sigmoid())\n",
    "        self.ca = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n",
    "                                Conv(decode_channels, decode_channels//16, kernel_size=1),\n",
    "                                nn.ReLU6(),\n",
    "                                Conv(decode_channels//16, decode_channels, kernel_size=1),\n",
    "                                nn.Sigmoid())\n",
    "\n",
    "        self.shortcut = ConvBN(decode_channels, decode_channels, kernel_size=1)\n",
    "        self.proj = SeparableConvBN(decode_channels, decode_channels, kernel_size=3)\n",
    "        self.act = nn.ReLU6()\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        # x = x.permute(0,3,1,2)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        weights = nn.ReLU()(self.weights)\n",
    "        fuse_weights = weights / (torch.sum(weights, dim=0) + self.eps)\n",
    "        x = fuse_weights[0] * self.pre_conv(res) + fuse_weights[1] * x\n",
    "        x = self.post_conv(x)\n",
    "        shortcut = self.shortcut(x)\n",
    "        pa = self.pa(x) * x\n",
    "        ca = self.ca(x) * x\n",
    "        x = pa + ca\n",
    "        x = self.proj(x) + shortcut\n",
    "        x = self.act(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AuxHead(nn.Module):\n",
    "    def __init__(self, in_channels=64, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBNReLU(in_channels, in_channels)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.conv_out = Conv(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        # x = x.permute(0,3,1,2)\n",
    "        feat = self.conv(x)\n",
    "        feat = self.drop(feat)\n",
    "        feat = self.conv_out(feat)\n",
    "        feat = F.interpolate(feat, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        return feat\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_channels=(64, 128, 256, 512),\n",
    "                 decode_channels=64,\n",
    "                 dropout=0.1,\n",
    "                 window_size=8,\n",
    "                 num_classes=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.pre_conv = ConvBN(encoder_channels[-1], decode_channels, kernel_size=1)\n",
    "        self.b4 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n",
    "\n",
    "        self.b3 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n",
    "        self.p3 = WF(encoder_channels[-2], decode_channels)\n",
    "\n",
    "        self.b2 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n",
    "        self.p2 = WF(encoder_channels[-3], decode_channels)\n",
    "\n",
    "        if self.training:\n",
    "            self.up4 = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "            self.up3 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "            self.aux_head = AuxHead(decode_channels, num_classes)\n",
    "\n",
    "        self.p1 = FeatureRefinementHead(encoder_channels[-4], decode_channels)\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(ConvBNReLU(decode_channels, decode_channels),\n",
    "                                               nn.Dropout2d(p=dropout, inplace=True),\n",
    "                                               Conv(decode_channels, num_classes, kernel_size=1))\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, res1, res2, res3, res4, h, w):\n",
    "        if self.training:\n",
    "            x = self.b4(self.pre_conv(res4))\n",
    "            h4 = self.up4(x)\n",
    "\n",
    "            x = self.p3(x, res3)\n",
    "            x = self.b3(x)\n",
    "            h3 = self.up3(x)\n",
    "\n",
    "            x = self.p2(x, res2)\n",
    "            x = self.b2(x)\n",
    "            h2 = x\n",
    "            x = self.p1(x, res1)\n",
    "            x = self.segmentation_head(x)\n",
    "            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n",
    "\n",
    "            ah = h4 + h3 + h2\n",
    "            ah = self.aux_head(ah, h, w)\n",
    "\n",
    "            return x, ah\n",
    "        else:\n",
    "            x = self.b4(self.pre_conv(res4))\n",
    "            x = self.p3(x, res3)\n",
    "            x = self.b3(x)\n",
    "\n",
    "            x = self.p2(x, res2)\n",
    "            x = self.b2(x)\n",
    "\n",
    "            x = self.p1(x, res1)\n",
    "\n",
    "            x = self.segmentation_head(x)\n",
    "            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def init_weight(self):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class UNetFormer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 decode_channels=64,\n",
    "                 dropout=0.1,\n",
    "                 backbone_name='swsl_resnet18',\n",
    "                 pretrained=True,\n",
    "                 window_size=8,\n",
    "                 num_classes=1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(backbone_name, features_only=True, output_stride=32,\n",
    "                                          out_indices=(1, 2, 3, 4), pretrained=pretrained)\n",
    "        encoder_channels = self.backbone.feature_info.channels()\n",
    "        self.decoder = Decoder(encoder_channels, decode_channels, dropout, window_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.permute(0,3,1,2)\n",
    "        h, w = x.size()[-2:]\n",
    "        res1, res2, res3, res4 = self.backbone(x)\n",
    "        if self.training:\n",
    "            x, ah = self.decoder(res1, res2, res3, res4, h, w)\n",
    "            return x, ah\n",
    "        else:\n",
    "            x = self.decoder(res1, res2, res3, res4, h, w)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKpZVjo2wGCu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seg_model = torch.load(\"./model(unetf1).pth\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU_-26g9f7PJ"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(seg_model, input_size=(3, 1024, 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpWMKSm2wRr-"
   },
   "source": [
    "### 2) Second Model - Gan (upsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hPxrjwX1FYF"
   },
   "source": [
    "#### 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsgx4le5zH8K"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAtHocL6aITm"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./test.csv')\n",
    "img_path = (test_data['img_path'])[0]\n",
    "display(Image(filename = img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzV-dJsQ0WTP"
   },
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQXIkirg0PIc"
   },
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WX4O5DwezOwj"
   },
   "outputs": [],
   "source": [
    "tmp_transform = A.Compose([A.Resize(1024, 1024)])\n",
    "\n",
    "test_image = cv2.imread(img_path)\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "result = tmp_transform(image = test_image)['image']\n",
    "\n",
    "tensor_to_image(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMPd6n6q4u8F"
   },
   "source": [
    "\n",
    "#### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wctt4jlcHqi_"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cv2 import dnn_superres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8Hr7rrRIRK2"
   },
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4ECsYibFdZr"
   },
   "outputs": [],
   "source": [
    "# Create an SR object\n",
    "sr = dnn_superres.DnnSuperResImpl_create()\n",
    "\n",
    "# Read image\n",
    "image = cv2.imread(img_path)\n",
    "\n",
    "# Read the desired model\n",
    "path = \"FSRCNN_x4.pb\"\n",
    "sr.readModel(path)\n",
    "\n",
    "# Set the desired model and scale to get correct pre- and post-processing\n",
    "sr.setModel(\"fsrcnn\", 4)\n",
    "\n",
    "# Upscale the image\n",
    "result = sr.upsample(image)\n",
    "print(result)\n",
    "\n",
    "# Save the image\n",
    "cv2.imwrite(\"./upscaled.png\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hR_2JZXIEYhk"
   },
   "outputs": [],
   "source": [
    "display(Image(filename = './upscaled.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3WX1y4ibXQo"
   },
   "outputs": [],
   "source": [
    "# Upscale the image\n",
    "result = sr.upsample(result)\n",
    "print(result)\n",
    "\n",
    "# Save the image\n",
    "cv2.imwrite(\"./upscaled.png\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ndo9an_XbrGL"
   },
   "outputs": [],
   "source": [
    "tmp2_transform = A.Compose([\n",
    "    A.Resize(1024, 1024),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFqnZb-sbv6d"
   },
   "outputs": [],
   "source": [
    "result = tmp2_transform(image = result)['image']\n",
    "\n",
    "# Save the image\n",
    "cv2.imwrite(\"./upscaled.png\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eY28nsfFJgTr"
   },
   "outputs": [],
   "source": [
    "display(Image(filename = './upscaled.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8m_OI7fcS7D"
   },
   "outputs": [],
   "source": [
    "display(Image(filename = './upscaled.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_2M-AYbx6jt"
   },
   "source": [
    "### 3) Model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R2jvIAhLVoc"
   },
   "outputs": [],
   "source": [
    "# # model 초기화\n",
    "model = UNetFormer().to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(8):  # 10 에폭 동안 학습합니다.\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(train_dataloader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs[0], masks.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwNHdJx4L4E5"
   },
   "source": [
    "### 4) valid (성능 평가를 위한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqVdDA1zyhH6"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    seg_model.eval()\n",
    "    result = []\n",
    "    for images, mask in tqdm(valid_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "        outputs = seg_model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6C2-bTSLtQl"
   },
   "source": [
    "### 5) 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMl-wJoxNF_5"
   },
   "outputs": [],
   "source": [
    "class SatelliteDatasetForTest(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = sr.upsample(image)\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = sr.upsample(image)\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k050sGUtMFo7"
   },
   "outputs": [],
   "source": [
    "test_transform = A.Compose([\n",
    "    A.Resize(1024, 1024),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xp2H822dL2JO"
   },
   "outputs": [],
   "source": [
    "test_dataset = SatelliteDataset(csv_file='./test.csv', transform=test_transform, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jCWDU62Ny2I"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    seg_model.eval()\n",
    "    result = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "        outputs = seg_model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6mtXMcw1hGd"
   },
   "source": [
    "## 4) 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJdtQIL8WnKq"
   },
   "source": [
    "### true_mask vs pred_mask 이미지 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwIN43JDKSpk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == -1:\n",
    "        return np.zeros(shape)\n",
    "\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ff_QbU2KpbJJ"
   },
   "outputs": [],
   "source": [
    "lst_input_img_path = list(valid['img_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TClN804NMZEi"
   },
   "outputs": [],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy_eO0lBt3ij"
   },
   "outputs": [],
   "source": [
    "test_data['img_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82fkKDDQWMlF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        if i == 0:\n",
    "            img = cv2.imread(display_list[i])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.uint8).copy()\n",
    "        if i == 1:\n",
    "            img = rle_decode(display_list[i], shape = (1024, 1024)) # shape 설정\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "display_list = [test_data['img_path'][9], result[9]]\n",
    "display(display_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4yZ5ZlOWoU5"
   },
   "source": [
    "### Dice Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsKqKb94tqtW"
   },
   "outputs": [],
   "source": [
    "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
    "    '''\n",
    "    Calculate Dice Score between two binary masks.\n",
    "    '''\n",
    "    intersection = np.sum(prediction * ground_truth)\n",
    "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
    "\n",
    "\n",
    "def calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224)) -> List[float]:\n",
    "    '''\n",
    "    Calculate Dice scores for a dataset.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Keep only the rows in the prediction dataframe that have matching img_ids in the ground truth dataframe\n",
    "    prediction_df = prediction_df[prediction_df.iloc[:, 0].isin(ground_truth_df.iloc[:, 0])]\n",
    "    prediction_df.index = range(prediction_df.shape[0])\n",
    "\n",
    "\n",
    "    # Extract the mask_rle columns\n",
    "    pred_mask_rle = prediction_df.iloc[:, 1]\n",
    "    gt_mask_rle = ground_truth_df.iloc[:, 1]\n",
    "\n",
    "\n",
    "    def calculate_dice(pred_rle, gt_rle):\n",
    "        pred_mask = rle_decode(pred_rle, img_shape)\n",
    "        gt_mask = rle_decode(gt_rle, img_shape)\n",
    "\n",
    "\n",
    "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
    "            return dice_score(pred_mask, gt_mask)\n",
    "        else:\n",
    "            return None  # No valid masks found, return None\n",
    "\n",
    "\n",
    "    dice_scores = Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n",
    "    )\n",
    "\n",
    "\n",
    "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
    "\n",
    "\n",
    "    return np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqV23oDA6dWc"
   },
   "outputs": [],
   "source": [
    "# ground_truth_df = valid.drop('img_path', axis=1)\n",
    "df = valid.drop('img_path', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emWwnB9S9nbo"
   },
   "outputs": [],
   "source": [
    "# valid_pred = {'img_id': ground_truth_df['img_id'], 'mask_rle': result}\n",
    "valid_pred = {'img_id': df['img_id'], 'mask_rle': result}\n",
    "prediction_df = pd.DataFrame(data = valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLSqYpHtUilF"
   },
   "outputs": [],
   "source": [
    "lst_ground_truth_rle = [rle_encode((validset.__getitem__(i))[1]) for i in range(len(valid))]\n",
    "valid_pred = {'img_id': df['img_id'], 'mask_rle': lst_ground_truth_rle}\n",
    "ground_truth_df = pd.DataFrame(data = valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i8bLTC0XTsk"
   },
   "outputs": [],
   "source": [
    "rle_decode(ground_truth_df['mask_rle'][4458]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vUfwAUDt7J0"
   },
   "outputs": [],
   "source": [
    "calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(1024, 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PydI60pCMV0F"
   },
   "source": [
    "### Class 별 IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr9o8gw7niol"
   },
   "outputs": [],
   "source": [
    "predNoBuildingIdx = list(filter(lambda x: result[x] == -1, range(len(result))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWNmfBJTaGX_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def generateConfusionMatrix(ground_truth_mask, pred_mask):\n",
    "    y_true = sum(rle_decode(ground_truth_mask).tolist(), [])\n",
    "    y_pred = sum(rle_decode(pred_mask).tolist(), [])\n",
    "    cMatrix = confusion_matrix(y_true, y_pred)\n",
    "    return cMatrix\n",
    "\n",
    "def generateConfusionMatrixLst(lst_ground_truth_rle, lst_pred_rle):\n",
    "    lst_cMatrix = Parallel(n_jobs=1)(delayed(generateConfusionMatrix)(lst_ground_truth_rle[i], result[i]) for i in range(len(lst_ground_truth_rle)))\n",
    "    return lst_cMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UwanclfbovH"
   },
   "outputs": [],
   "source": [
    "Lst_cMatrix = generateConfusionMatrixLst(lst_ground_truth_rle, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqQtVfvKeMmJ"
   },
   "outputs": [],
   "source": [
    "Lst_cMatrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbcrKuzDfhPF"
   },
   "outputs": [],
   "source": [
    "def IoU(cMatrix):\n",
    "    Intersection = cMatrix.diagonal()\n",
    "    Union11 = cMatrix.sum(axis = 0)[0] + cMatrix[0][1]\n",
    "    Union22 = cMatrix.sum(axis = 0)[1] + cMatrix[1][0]\n",
    "    Union = np.array([Union11, Union22])\n",
    "    return Intersection / Union\n",
    "\n",
    "# 전체 이미지 IoU 수치에 대하여 평균냄.\n",
    "def totalIoU(lst_cMatrix):\n",
    "    totalIoU = np.array([0, 0], dtype = 'float64')\n",
    "    for cMat in lst_cMatrix:\n",
    "        totalIoU += IoU(cMat)\n",
    "    return totalIoU / len(lst_cMatrix)\n",
    "\n",
    "def eachIoU(lst_cMatrix):\n",
    "    eachIoU = []\n",
    "    for cMat in lst_cMatrix:\n",
    "        eachIoU.append(IoU(cMat))\n",
    "    return eachIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQajFnHwgZXH"
   },
   "outputs": [],
   "source": [
    "IoU(Lst_cMatrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EegcalN9hhR6"
   },
   "outputs": [],
   "source": [
    "totalIoU(Lst_cMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drVZ9YBWimB6"
   },
   "outputs": [],
   "source": [
    "totaliou = totalIoU(Lst_cMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8V3_cT_co5S"
   },
   "outputs": [],
   "source": [
    "def printClassScores(totaliou):\n",
    "    label = ['background', 'building']\n",
    "    print('classes          IoU      nIoU')\n",
    "    print('--------------------------------')\n",
    "    for i, iou in enumerate(totaliou):\n",
    "        labelName = label[i]\n",
    "        iouStr = f'{iou:>5.3f}'\n",
    "        niouStr = 'empty'\n",
    "        print('{:<14}: '.format(labelName) + iouStr + '    ' + niouStr)\n",
    "    print('--------------------------------')\n",
    "    print(f'Score Average : {(np.sum(totaliou) / 2):>5.3f}' + '    ' + niouStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8FbFF_djRwC"
   },
   "outputs": [],
   "source": [
    "printClassScores(totaliou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86Qao5IfNhfX"
   },
   "source": [
    "# 제출 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zJknfBNOKZ3"
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z39rrxjOOKx1"
   },
   "outputs": [],
   "source": [
    "submit.to_csv('./submit(unetf1).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
