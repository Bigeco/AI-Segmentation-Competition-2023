{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLoUbbWfnnTQ"
   },
   "source": [
    "# 0. 초기 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LjVNJocZ2IR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Pytorch에서 gpu를 사용하는 방법.\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSifeRI5NZIR"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import tifffile as tiff\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0Ucurv9N9xj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcspYaHHKoOq"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-KLMtHmhco7"
   },
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yt_htC4wnOFZ"
   },
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2tmmvGUX2so"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duPMsWN0XKjC"
   },
   "outputs": [],
   "source": [
    "train_img_dir = './pro_data/data_for_training_and_testing/train/images/'\n",
    "train_mask_dir = './pro_data/data_for_training_and_testing/train/masks/'\n",
    "\n",
    "valid_img_dir = './pro_data/data_for_training_and_testing/val/images/'\n",
    "valid_mask_dir = './pro_data/data_for_training_and_testing/val/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_img = sorted(os.listdir(train_img_dir))\n",
    "data_train_mask = sorted(os.listdir(train_mask_dir))\n",
    "\n",
    "data_val_img = sorted(os.listdir(valid_img_dir))\n",
    "data_val_mask = sorted(os.listdir(valid_mask_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'img_path': data_train_img, 'mask_path': data_train_mask})\n",
    "df_valid = pd.DataFrame({'img_path': data_val_img, 'mask_path': data_val_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNIYghTtZqOO"
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsScGmliaF8G"
   },
   "source": [
    "### Custom Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQOJM-cvL_kc"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cv2 import dnn_superres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK1PxG8UMAxC"
   },
   "outputs": [],
   "source": [
    "# Create an SR object\n",
    "sr = dnn_superres.DnnSuperResImpl_create()\n",
    "\n",
    "# Read the desired model\n",
    "path = \"FSRCNN_x4.pb\"\n",
    "sr.readModel(path)\n",
    "\n",
    "# Set the desired model and scale to get correct pre- and post-processing\n",
    "sr.setModel(\"fsrcnn\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo_qzaXXMUzb"
   },
   "outputs": [],
   "source": [
    "tmp2_transform = A.Compose([\n",
    "    A.Resize(896, 896),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Glnq70NyXKjJ"
   },
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, dataset, lst_path, transform=None, infer=False):\n",
    "        self.data = dataset\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "        self.lst_path = lst_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.lst_path[0] + self.data.iloc[idx, 0]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = sr.upsample(image)\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_path = self.lst_path[1] + self.data.iloc[idx, 1]\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "\n",
    "        if self.transform:\n",
    "#             image = sr.upsample(image)\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhwHCGNfOCky"
   },
   "outputs": [],
   "source": [
    "train_img_dir + df_train.iloc[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwty-NSdbQTB"
   },
   "source": [
    "**[해야될 것] transform 바꿔보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAtvvUOxXKjJ"
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    # A.Resize(224, 224)  # 적용 안해도됨. 이미 주어진 이미지가 224x224\n",
    "    A.Normalize(),        # 기존\n",
    "#     A.HorizontalFlip(p=0.5),   # 새로 추가\n",
    "#     A.VerticalFlip(p=0.5),     # 새로 추가\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    ToTensorV2()          # 기존\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    # A.Resize(224, 224)  # 적용 안해도됨. 이미 주어진 이미지가 224x224\n",
    "    A.Normalize(),        # 기존\n",
    "#     A.HorizontalFlip(p=0.5),   # 새로 추가\n",
    "#     A.VerticalFlip(p=0.5),     # 새로 추가\n",
    "#     A.RandomRotate90(p=0.5),\n",
    "    ToTensorV2()          # 기존\n",
    "])\n",
    "\n",
    "trainset = SatelliteDataset(dataset = df_train, transform=transform, lst_path = [train_img_dir, train_mask_dir])\n",
    "validset = SatelliteDataset(dataset = df_valid, transform=valid_transform, lst_path = [valid_img_dir, valid_mask_dir])\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=48)\n",
    "valid_dataloader = DataLoader(validset, batch_size=16, shuffle=False, num_workers=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer: Optimizer, k: int = 5, alpha: float = 0.5):\n",
    "        \"\"\"@TODO: Docs. Contribution is welcome.\"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.defaults = self.optimizer.defaults\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "\n",
    "\n",
    "    def update(self, group):\n",
    "        \"\"\"@TODO: Docs. Contribution is welcome.\"\"\"\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "\n",
    "\n",
    "    def update_lookahead(self):\n",
    "        \"\"\"@TODO: Docs. Contribution is welcome.\"\"\"\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        \"\"\"Makes optimizer step.\n",
    "\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates\n",
    "                the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"@TODO: Docs. Contribution is welcome.\"\"\"\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"@TODO: Docs. Contribution is welcome.\"\"\"\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        \"\"\"@TODO: Docs. Contribution is welcome.\"\"\"\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_from_params(\n",
    "        cls, params: Dict, base_optimizer_params: Dict = None, **kwargs,\n",
    "    ) -> \"Lookahead\":\n",
    "        \"\"\"@TODO: Docs. Contribution is welcome.\"\"\"\n",
    "        from catalyst.dl.registry import OPTIMIZERS\n",
    "\n",
    "        base_optimizer = OPTIMIZERS.get_from_params(\n",
    "            params=params, **base_optimizer_params\n",
    "        )\n",
    "        optimizer = cls(optimizer=base_optimizer, **kwargs)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import utils\n",
    "from timm.scheduler.poly_lr import PolyLRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class MaxPoolLayer(nn.Sequential):\n",
    "    def __init__(self, kernel_size=3, dilation=1, stride=1):\n",
    "        super(MaxPoolLayer, self).__init__(\n",
    "            nn.MaxPool2d(kernel_size=kernel_size, dilation=dilation, stride=stride,\n",
    "                         padding=((stride - 1) + dilation * (kernel_size - 1)) // 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class AvgPoolLayer(nn.Sequential):\n",
    "    def __init__(self, kernel_size=3, stride=1):\n",
    "        super(AvgPoolLayer, self).__init__(\n",
    "            nn.AvgPool2d(kernel_size=kernel_size, stride=stride,\n",
    "                         padding=(kernel_size-1)//2)\n",
    "        )\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n",
    "        super(ConvBN, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n",
    "            norm_layer(out_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "class Conv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, bias=False):\n",
    "        super(Conv, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n",
    "                 norm_layer=nn.BatchNorm2d):\n",
    "        super(SeparableConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConvBN(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n",
    "                 norm_layer=nn.BatchNorm2d):\n",
    "        super(SeparableConvBN, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            norm_layer(out_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1):\n",
    "        super(SeparableConv, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class TransposeConvBNReLu(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2, norm_layer=nn.BatchNorm2d):\n",
    "        super(TransposeConvBNReLu, self).__init__(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "class TransposeConvBN(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2, norm_layer=nn.BatchNorm2d):\n",
    "        super(TransposeConvBN, self).__init__(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
    "            norm_layer(out_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "class TransposeConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2):\n",
    "        super(TransposeConv, self).__init__(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        )\n",
    "\n",
    "\n",
    "class PyramidPool(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, pool_size=1, norm_layer=nn.BatchNorm2d):\n",
    "        super(PyramidPool, self).__init__(\n",
    "            nn.AdaptiveAvgPool2d(pool_size),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "        # print(self.relative_position_bias_table.shape)\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        # print(coords_h, coords_w)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        # print(torch.meshgrid([coords_h, coords_w]))\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        # print(coords_flatten)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        # print(relative_coords[0,7,:])\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        # print(relative_coords[:, :, 0], relative_coords[:, :, 1])\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\" Forward function.\n",
    "\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        # print(B_,N,C)\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        # print(attn.shape)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        # print(relative_position_bias.unsqueeze(0))\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \"\"\" Forward function.\n",
    "\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "            mask_matrix: Attention mask for cyclic shift.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\" Patch Merging Layer\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of feature channels\n",
    "        depth (int): Depths of this stage.\n",
    "        num_heads (int): Number of attention head.\n",
    "        window_size (int): Local window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "            else:\n",
    "                x = blk(x, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\" Swin Transformer backbone.\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        pretrain_img_size (int): Input image size for training the pretrained models,\n",
    "            used in absolute postion embedding. Default 224.\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        depths (tuple[int]): Depths of each Swin Transformer stage.\n",
    "        num_heads (tuple[int]): Number of attention head of each stage.\n",
    "        window_size (int): Window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop_rate (float): Dropout rate.\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0.\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False.\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True.\n",
    "        out_indices (Sequence[int]): Output from which stages.\n",
    "        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n",
    "            -1 means not freezing any parameters.\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrain_img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=128,\n",
    "                 depths=[2, 2, 18, 2],\n",
    "                 num_heads=[4, 8, 16, 32],\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.3,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 ape=False,\n",
    "                 patch_norm=True,\n",
    "                 out_indices=(0, 1, 2, 3),\n",
    "                 frozen_stages=-1,\n",
    "                 use_checkpoint=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.out_indices = out_indices\n",
    "        self.frozen_stages = frozen_stages\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            pretrain_img_size = to_2tuple(pretrain_img_size)\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1]]\n",
    "\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1]))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2 ** i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n",
    "        self.num_features = num_features\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # add a norm layer for each output\n",
    "        for i_layer in out_indices:\n",
    "            layer = norm_layer(num_features[i_layer])\n",
    "            layer_name = f'norm{i_layer}'\n",
    "            self.add_module(layer_name, layer)\n",
    "\n",
    "        self._freeze_stages()\n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        if self.frozen_stages >= 0:\n",
    "            self.patch_embed.eval()\n",
    "            for param in self.patch_embed.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 1 and self.ape:\n",
    "            self.absolute_pos_embed.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 2:\n",
    "            self.pos_drop.eval()\n",
    "            for i in range(0, self.frozen_stages - 1):\n",
    "                m = self.layers[i]\n",
    "                m.eval()\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self.patch_embed(x)\n",
    "        # print('patch_embed', x.size())\n",
    "\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "        if self.ape:\n",
    "            # interpolate the position embedding to the corresponding size\n",
    "            absolute_pos_embed = F.interpolate(self.absolute_pos_embed, size=(Wh, Ww), mode='bicubic')\n",
    "            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww C\n",
    "        else:\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            layer = self.layers[i]\n",
    "            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "\n",
    "            if i in self.out_indices:\n",
    "                norm_layer = getattr(self, f'norm{i}')\n",
    "                x_out = norm_layer(x_out)\n",
    "\n",
    "                out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "                outs.append(out)\n",
    "                # print('layer{} out size {}'.format(i, out.size()))\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Convert the models into training mode while keep layers freezed.\"\"\"\n",
    "        super(SwinTransformer, self).train(mode)\n",
    "        self._freeze_stages()\n",
    "\n",
    "\n",
    "def l2_norm(x):\n",
    "    return torch.einsum(\"bcn, bn->bcn\", x, 1 / torch.norm(x, p=2, dim=-2))\n",
    "\n",
    "\n",
    "class SharedSpatialAttention(nn.Module):\n",
    "    def __init__(self, in_places, eps=1e-6):\n",
    "        super(SharedSpatialAttention, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.in_places = in_places\n",
    "        self.l2_norm = l2_norm\n",
    "        self.eps = eps\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_places, out_channels=in_places // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_places, out_channels=in_places // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_places, out_channels=in_places, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the feature map to the queries and keys\n",
    "        batch_size, chnnels, width, height = x.shape\n",
    "        Q = self.query_conv(x).view(batch_size, -1, width * height)\n",
    "        K = self.key_conv(x).view(batch_size, -1, width * height)\n",
    "        V = self.value_conv(x).view(batch_size, -1, width * height)\n",
    "\n",
    "        Q = self.l2_norm(Q).permute(-3, -1, -2)\n",
    "        # print('q', Q.shape)\n",
    "        K = self.l2_norm(K)\n",
    "        # print('k', K.shape)\n",
    "\n",
    "        tailor_sum = 1 / (width * height + torch.einsum(\"bnc, bc->bn\", Q, torch.sum(K, dim=-1) + self.eps))\n",
    "        # print('tailor_sum', tailor_sum.shape)\n",
    "        value_sum = torch.einsum(\"bcn->bc\", V).unsqueeze(-1)\n",
    "        # print('value_sum', value_sum.shape)\n",
    "        value_sum = value_sum.expand(-1, chnnels, width * height)\n",
    "        # print('value_sum', value_sum.shape)\n",
    "\n",
    "        matrix = torch.einsum('bmn, bcn->bmc', K, V)\n",
    "        # print('matrix',matrix.shape)\n",
    "        matrix_sum = value_sum + torch.einsum(\"bnm, bmc->bcn\", Q, matrix)\n",
    "        # print('matrix_sum', matrix_sum.shape)\n",
    "\n",
    "        weight_value = torch.einsum(\"bcn, bn->bcn\", matrix_sum, tailor_sum)\n",
    "        # print('weight_value', weight_value.shape)\n",
    "        weight_value = weight_value.view(batch_size, chnnels, height, width)\n",
    "\n",
    "        return (x + self.gamma * weight_value).contiguous()\n",
    "\n",
    "\n",
    "class SharedChannelAttention(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(SharedChannelAttention, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.l2_norm = l2_norm\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, chnnels, width, height = x.shape\n",
    "        Q = x.view(batch_size, chnnels, -1)\n",
    "        K = x.view(batch_size, chnnels, -1)\n",
    "        V = x.view(batch_size, chnnels, -1)\n",
    "\n",
    "        Q = self.l2_norm(Q)\n",
    "        K = self.l2_norm(K).permute(-3, -1, -2)\n",
    "\n",
    "        tailor_sum = 1 / (width * height + torch.einsum(\"bnc, bn->bc\", K, torch.sum(Q, dim=-2) + self.eps))\n",
    "        value_sum = torch.einsum(\"bcn->bn\", V).unsqueeze(-1).permute(0, 2, 1)\n",
    "        value_sum = value_sum.expand(-1, chnnels, width * height)\n",
    "        matrix = torch.einsum('bcn, bnm->bcm', V, K)\n",
    "        matrix_sum = value_sum + torch.einsum(\"bcm, bmn->bcn\", matrix, Q)\n",
    "\n",
    "        weight_value = torch.einsum(\"bcn, bc->bcn\", matrix_sum, tailor_sum)\n",
    "        weight_value = weight_value.view(batch_size, chnnels, height, width)\n",
    "\n",
    "        return (x + self.gamma * weight_value).contiguous()\n",
    "\n",
    "\n",
    "class DownConnection(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=2):\n",
    "        super(DownConnection, self).__init__()\n",
    "        self.convbn1 = ConvBN(inplanes, planes, kernel_size=3, stride=1)\n",
    "        self.convbn2 = ConvBN(planes, planes, kernel_size=3, stride=stride)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = ConvBN(inplanes, planes, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.convbn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.convbn2(x)\n",
    "        x = x + self.downsample(residual)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DCFAM(nn.Module):\n",
    "    def __init__(self, encoder_channels=(96, 192, 384, 768), atrous_rates=(6, 12)):\n",
    "        super(DCFAM, self).__init__()\n",
    "        rate_1, rate_2 = tuple(atrous_rates)\n",
    "        self.conv4 = Conv(encoder_channels[3], encoder_channels[3], kernel_size=1)\n",
    "        self.conv1 = Conv(encoder_channels[0], encoder_channels[0], kernel_size=1)\n",
    "        self.lf4 = nn.Sequential(SeparableConvBNReLU(encoder_channels[-1], encoder_channels[-2], dilation=rate_1),\n",
    "                                 nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                 SeparableConvBNReLU(encoder_channels[-2], encoder_channels[-3], dilation=rate_2),\n",
    "                                 nn.UpsamplingNearest2d(scale_factor=2))\n",
    "        self.lf3 = nn.Sequential(SeparableConvBNReLU(encoder_channels[-2], encoder_channels[-3], dilation=rate_1),\n",
    "                                 nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                 SeparableConvBNReLU(encoder_channels[-3], encoder_channels[-4], dilation=rate_2),\n",
    "                                 nn.UpsamplingNearest2d(scale_factor=2))\n",
    "\n",
    "        self.ca = SharedChannelAttention()\n",
    "        self.pa = SharedSpatialAttention(in_places=encoder_channels[2])\n",
    "        self.down12 = DownConnection(encoder_channels[0], encoder_channels[1])\n",
    "        self.down231 = DownConnection(encoder_channels[1], encoder_channels[2])\n",
    "        self.down232 = DownConnection(encoder_channels[1], encoder_channels[2])\n",
    "        self.down34 = DownConnection(encoder_channels[2], encoder_channels[3])\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        out4 = self.conv4(x4) + self.down34(self.pa(self.down232(x2)))\n",
    "        out3 = self.pa(x3) + self.down231(self.ca(self.down12(x1)))\n",
    "        out2 = self.ca(x2) + self.lf4(out4)\n",
    "        del out4\n",
    "        out1 = self.lf3(out3) + self.conv1(x1)\n",
    "        del out3\n",
    "\n",
    "        return out1, out2\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_channels=(96, 192, 384, 768),\n",
    "                 dropout=0.05,\n",
    "                 atrous_rates=(6, 12),\n",
    "                 num_classes=6):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dcfam = DCFAM(encoder_channels, atrous_rates)\n",
    "        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            ConvBNReLU(encoder_channels[0], encoder_channels[0]),\n",
    "            Conv(encoder_channels[0], num_classes, kernel_size=1),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=4))\n",
    "        self.up = nn.Sequential(\n",
    "            ConvBNReLU(encoder_channels[1], encoder_channels[0]),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        )\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        out1, out2 = self.dcfam(x1, x2, x3, x4)\n",
    "        x = out1 + self.up(out2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.segmentation_head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def init_weight(self):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class DCSwin(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_channels=(96, 192, 384, 768),\n",
    "                 dropout=0.05,\n",
    "                 atrous_rates=(6, 12),\n",
    "                 num_classes=6,\n",
    "                 embed_dim=128,\n",
    "                 depths=(2, 2, 18, 2),\n",
    "                 num_heads=(4, 8, 16, 32),\n",
    "                 frozen_stages=2):\n",
    "        super(DCSwin, self).__init__()\n",
    "        self.backbone = SwinTransformer(embed_dim=embed_dim, depths=depths, num_heads=num_heads, frozen_stages=frozen_stages)\n",
    "        self.decoder = Decoder(encoder_channels, dropout, atrous_rates, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4 = self.backbone(x)\n",
    "        x = self.decoder(x1, x2, x3, x4)\n",
    "        return x\n",
    "\n",
    "\n",
    "def dcswin_base(pretrained=True, num_classes=4, weight_path='pretrain_weights/stseg_base.pth'):\n",
    "    # pretrained weights are load from official repo of Swin Transformer\n",
    "    model = DCSwin(encoder_channels=(128, 256, 512, 1024),\n",
    "                   num_classes=num_classes,\n",
    "                   embed_dim=128,\n",
    "                   depths=(2, 2, 18, 2),\n",
    "                   num_heads=(4, 8, 16, 32),\n",
    "                   frozen_stages=2)\n",
    "    if pretrained and weight_path is not None:\n",
    "        old_dict = torch.load(weight_path)['state_dict']\n",
    "        model_dict = model.state_dict()\n",
    "        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n",
    "        model_dict.update(old_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def dcswin_small(pretrained=True, num_classes=4, weight_path='pretrain_weights/stseg_small.pth'):\n",
    "    model = DCSwin(encoder_channels=(96, 192, 384, 768),\n",
    "                   num_classes=num_classes,\n",
    "                   embed_dim=96,\n",
    "                   depths=(2, 2, 18, 2),\n",
    "                   num_heads=(3, 6, 12, 24),\n",
    "                   frozen_stages=2)\n",
    "    if pretrained and weight_path is not None:\n",
    "        old_dict = torch.load(weight_path)['state_dict']\n",
    "        model_dict = model.state_dict()\n",
    "        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n",
    "        model_dict.update(old_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def dcswin_tiny(pretrained=True, num_classes=4, weight_path='pretrain_weights/stseg_tiny.pth'):\n",
    "    model = DCSwin(encoder_channels=(96, 192, 384, 768),\n",
    "                   num_classes=num_classes,\n",
    "                   embed_dim=96,\n",
    "                   depths=(2, 2, 6, 2),\n",
    "                   num_heads=(3, 6, 12, 24),\n",
    "                   frozen_stages=2)\n",
    "    if pretrained and weight_path is not None:\n",
    "        old_dict = torch.load(weight_path)['state_dict']\n",
    "        model_dict = model.state_dict()\n",
    "        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n",
    "        model_dict.update(old_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hparam\n",
    "max_epoch = 30\n",
    "ignore_index = 0 \n",
    "train_batch_size = 8\n",
    "val_batch_size = 4\n",
    "lr = 1e-3\n",
    "weight_decay = 2.5e-4\n",
    "backbone_lr = 1e-4\n",
    "backbone_weight_decay = 2.5e-4\n",
    "num_classes = 1\n",
    "classes = ['background', 'building']\n",
    "\n",
    "weights_name = \"dcswin-small-1024-ms-512crop-e30\"\n",
    "weights_path = \"model_weights/potsdam/{}\".format(weights_name)\n",
    "test_weights_name = \"dcswin-small-1024-ms-512crop-e30\"\n",
    "log_name = 'potsdam/{}'.format(weights_name)\n",
    "monitor = 'val_F1'\n",
    "monitor_mode = 'max'\n",
    "save_top_k = 1\n",
    "save_last = False\n",
    "check_val_every_n_epoch = 1\n",
    "pretrained_ckpt_path = None # the path for the pretrained model weight\n",
    "gpus = 'auto'  # default or gpu ids:[0] or gpu nums: 2, more setting can refer to pytorch_lightning\n",
    "resume_ckpt_path = None  # whether continue training with the checkpoint, default None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define the network\n",
    "model = dcswin_small(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.DataParallel(model, device_ids = [2, 3])\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, List, Union\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends\n",
    "from torch.backends import cudnn\n",
    "\n",
    "# from catalyst.utils.tools.typing import Model\n",
    "\n",
    "# from pyhelpers.ops import merge_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.modules.module.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dicts(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Compare the difference between two dictionaries.\n",
    "\n",
    "    See also [`OPS-CD-1 <https://stackoverflow.com/questions/23177439>`_].\n",
    "\n",
    "    :param dict1: a dictionary\n",
    "    :type dict1: dict\n",
    "    :param dict2: another dictionary\n",
    "    :type dict2: dict\n",
    "    :return: in comparison to ``dict1``, the main difference on ``dict2``, including:\n",
    "        modified items, keys that are the same, keys where values remain unchanged, new keys and\n",
    "        keys that are removed\n",
    "    :rtype: typing.Tuple[dict, list]\n",
    "\n",
    "    **Examples**::\n",
    "\n",
    "        >>> from pyhelpers.ops import compare_dicts\n",
    "\n",
    "        >>> d1 = {'a': 1, 'b': 2, 'c': 3}\n",
    "        >>> d2 = {'b': 2, 'c': 4, 'd': [5, 6]}\n",
    "\n",
    "        >>> items_modified, k_shared, k_unchanged, k_new, k_removed = compare_dicts(d1, d2)\n",
    "        >>> items_modified\n",
    "        {'c': [3, 4]}\n",
    "        >>> k_shared\n",
    "        ['b', 'c']\n",
    "        >>> k_unchanged\n",
    "        ['b']\n",
    "        >>> k_new\n",
    "        ['d']\n",
    "        >>> k_removed\n",
    "        ['a']\n",
    "    \"\"\"\n",
    "\n",
    "    dk1, dk2 = map(lambda x: set(x.keys()), (dict1, dict2))\n",
    "\n",
    "    shared_keys = dk1.intersection(dk2)\n",
    "\n",
    "    added_keys, removed_keys = list(dk2 - dk1), list(dk1 - dk2)\n",
    "\n",
    "    modified_items = {k: [dict1[k], dict2[k]] for k in shared_keys if dict1[k] != dict2[k]}\n",
    "    unchanged_keys = list(set(k for k in shared_keys if dict1[k] == dict2[k]))\n",
    "\n",
    "    return modified_items, list(shared_keys), unchanged_keys, added_keys, removed_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(*dicts):\n",
    "    \"\"\"\n",
    "    Merge multiple dictionaries.\n",
    "\n",
    "    :param dicts: (one or) multiple dictionaries\n",
    "    :type dicts: dict\n",
    "    :return: a single dictionary containing all elements of the input\n",
    "    :rtype: dict\n",
    "\n",
    "    **Examples**::\n",
    "\n",
    "        >>> from pyhelpers.ops import merge_dicts\n",
    "\n",
    "        >>> dict_a = {'a': 1}\n",
    "        >>> dict_b = {'b': 2}\n",
    "        >>> dict_c = {'c': 3}\n",
    "\n",
    "        >>> merged_dict = merge_dicts(dict_a, dict_b, dict_c)\n",
    "        >>> merged_dict\n",
    "        {'a': 1, 'b': 2, 'c': 3}\n",
    "\n",
    "        >>> dict_c_ = {'c': 4}\n",
    "        >>> merged_dict = merge_dicts(merged_dict, dict_c_)\n",
    "        >>> merged_dict\n",
    "        {'a': 1, 'b': 2, 'c': [3, 4]}\n",
    "\n",
    "        >>> dict_1 = merged_dict\n",
    "        >>> dict_2 = {'b': 2, 'c': 4, 'd': [5, 6]}\n",
    "        >>> merged_dict = merge_dicts(dict_1, dict_2)\n",
    "        >>> merged_dict\n",
    "        {'a': 1, 'b': 2, 'c': [[3, 4], 4], 'd': [5, 6]}\n",
    "    \"\"\"\n",
    "\n",
    "    new_dict = {}\n",
    "    for d in dicts:\n",
    "        d_ = d.copy()\n",
    "        # dk1, dk2 = map(lambda x: set(x.keys()), (new_dict, d_))\n",
    "        # modified = {k: [new_dict[k], d_[k]] for k in dk1.intersection(dk2) if new_dict[k] != d_[k]}\n",
    "        modified_items, _, _, _, _, = compare_dicts(new_dict, d_)\n",
    "\n",
    "        if bool(modified_items):\n",
    "            new_dict.update(modified_items)\n",
    "            for k_ in modified_items.keys():\n",
    "                remove_dict_keys(d_, k_)\n",
    "\n",
    "        new_dict.update(d_)\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model_params(\n",
    "    model: torch.nn.modules.module.Module,\n",
    "    layerwise_params: Dict[str, dict] = None,\n",
    "    no_bias_weight_decay: bool = True,\n",
    "    lr_scaling: float = 1.0,\n",
    ") -> List[Union[torch.nn.Parameter, dict]]:\n",
    "    \"\"\"Gains model parameters for ``torch.optim.Optimizer``.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to process\n",
    "        layerwise_params (Dict): Order-sensitive dict where\n",
    "            each key is regex pattern and values are layer-wise options\n",
    "            for layers matching with a pattern\n",
    "        no_bias_weight_decay (bool): If true, removes weight_decay\n",
    "            for all ``bias`` parameters in the model\n",
    "        lr_scaling (float): layer-wise learning rate scaling,\n",
    "            if 1.0, learning rates will not be scaled\n",
    "\n",
    "    Returns:\n",
    "        iterable: parameters for an optimizer\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> model = catalyst.contrib.models.segmentation.ResnetUnet()\n",
    "        >>> layerwise_params = collections.OrderedDict([\n",
    "        >>>     (\"conv1.*\", dict(lr=0.001, weight_decay=0.0003)),\n",
    "        >>>     (\"conv.*\", dict(lr=0.002))\n",
    "        >>> ])\n",
    "        >>> params = process_model_params(model, layerwise_params)\n",
    "        >>> optimizer = torch.optim.Adam(params, lr=0.0003)\n",
    "\n",
    "    \"\"\"\n",
    "    params = list(model.named_parameters())\n",
    "    layerwise_params = layerwise_params or collections.OrderedDict()\n",
    "\n",
    "    model_params = []\n",
    "    for name, parameters in params:\n",
    "        options = {}\n",
    "        for pattern, options_ in layerwise_params.items():\n",
    "            if re.match(pattern, name) is not None:\n",
    "                # all new LR rules write on top of the old ones\n",
    "                options = merge_dicts(options, options_)\n",
    "\n",
    "        # no bias decay from https://arxiv.org/abs/1812.01187\n",
    "        if no_bias_weight_decay and name.endswith(\"bias\"):\n",
    "            options[\"weight_decay\"] = 0.0\n",
    "\n",
    "        # lr linear scaling from https://arxiv.org/pdf/1706.02677.pdf\n",
    "        if \"lr\" in options:\n",
    "            options[\"lr\"] *= lr_scaling\n",
    "\n",
    "        model_params.append({\"params\": parameters, **options})\n",
    "\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "layerwise_params = {\"backbone.*\": dict(lr=backbone_lr, weight_decay=backbone_weight_decay)}\n",
    "net_params = process_model_params(model, layerwise_params=layerwise_params)\n",
    "base_optimizer = torch.optim.AdamW(net_params, lr=lr, weight_decay=weight_decay)\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqOjvz5pXKjI"
   },
   "source": [
    "# 3. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bhd9eN1bYB9"
   },
   "source": [
    "**[해야될 것] 자기꺼 모델로 수정하는데 이미지 사이즈 224x224에 맞아야됨.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0ntq4eccChO"
   },
   "source": [
    "**[선택] loss와 optimizer 함수 바꿔보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self, num_class):\n",
    "        self.num_class = num_class\n",
    "        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def get_tp_fp_tn_fn(self):\n",
    "        tp = np.diag(self.confusion_matrix)\n",
    "        fp = self.confusion_matrix.sum(axis=0) - np.diag(self.confusion_matrix)\n",
    "        fn = self.confusion_matrix.sum(axis=1) - np.diag(self.confusion_matrix)\n",
    "        tn = np.diag(self.confusion_matrix).sum() - np.diag(self.confusion_matrix)\n",
    "        return tp, fp, tn, fn\n",
    "\n",
    "    def Precision(self):\n",
    "        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n",
    "        precision = tp / (tp + fp)\n",
    "        return precision\n",
    "\n",
    "    def Recall(self):\n",
    "        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n",
    "        recall = tp / (tp + fn)\n",
    "        return recall\n",
    "\n",
    "    def F1(self):\n",
    "        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n",
    "        Precision = tp / (tp + fp)\n",
    "        Recall = tp / (tp + fn)\n",
    "        F1 = (2.0 * Precision * Recall) / (Precision + Recall)\n",
    "        return F1\n",
    "\n",
    "    def OA(self):\n",
    "        OA = np.diag(self.confusion_matrix).sum() / (self.confusion_matrix.sum() + self.eps)\n",
    "        return OA\n",
    "\n",
    "    def Intersection_over_Union(self):\n",
    "        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n",
    "        IoU = tp / (tp + fn + fp)\n",
    "        return IoU\n",
    "\n",
    "    def Dice(self):\n",
    "        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n",
    "        Dice = 2 * tp / ((tp + fp) + (tp + fn))\n",
    "        return Dice\n",
    "\n",
    "    def Pixel_Accuracy_Class(self):\n",
    "        #         TP                                  TP+FP\n",
    "        Acc = np.diag(self.confusion_matrix) / (self.confusion_matrix.sum(axis=0) + self.eps)\n",
    "        return Acc\n",
    "\n",
    "    def Frequency_Weighted_Intersection_over_Union(self):\n",
    "        freq = np.sum(self.confusion_matrix, axis=1) / (np.sum(self.confusion_matrix) + self.eps)\n",
    "        iou = self.Intersection_over_Union()\n",
    "        FWIoU = (freq[freq > 0] * iou[freq > 0]).sum()\n",
    "        return FWIoU\n",
    "\n",
    "    def _generate_matrix(self, gt_image, pre_image):\n",
    "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
    "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
    "        count = np.bincount(label, minlength=self.num_class ** 2)\n",
    "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
    "        return confusion_matrix\n",
    "\n",
    "    def add_batch(self, gt_image, pre_image):\n",
    "        assert gt_image.shape == pre_image.shape, 'pre_image shape {}, gt_image shape {}'.format(pre_image.shape,\n",
    "                                                                                                 gt_image.shape)\n",
    "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
    "\n",
    "    def reset(self):\n",
    "        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     gt = np.array([[0, 2, 1],\n",
    "#                    [1, 2, 1],\n",
    "#                    [1, 0, 1]])\n",
    "\n",
    "#     pre = np.array([[0, 1, 1],\n",
    "#                    [2, 0, 1],\n",
    "#                    [1, 1, 1]])\n",
    "\n",
    "#     eval = Evaluator(num_class=3)\n",
    "#     eval.add_batch(gt, pre)\n",
    "#     print(eval.confusion_matrix)\n",
    "#     print(eval.get_tp_fp_tn_fn())\n",
    "#     print(eval.Precision())\n",
    "#     print(eval.Recall())\n",
    "#     print(eval.Intersection_over_Union())\n",
    "#     print(eval.OA())\n",
    "#     print(eval.F1())\n",
    "#     print(eval.Frequency_Weighted_Intersection_over_Union())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_epoch = []\n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(30):  # 10 에폭 동안 학습합니다.\n",
    "    # ===================== Train ===================== #\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(train_dataloader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    result = f'(train) Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader)}'\n",
    "    list_epoch.append(epoch)\n",
    "    list_train_loss.append(epoch_loss/len(train_dataloader))\n",
    "    \n",
    "    # ===================== Validation ===================== #\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(valid_dataloader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks.unsqueeze(1))\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    list_val_loss.append(epoch_loss/len(valid_dataloader))\n",
    "    \n",
    "    print(result)    \n",
    "    print(f'(valid) Epoch {epoch+1}, Loss: {epoch_loss/len(valid_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fia1b9fLXKjM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# ====== Loss Fluctuation ====== #\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
    "ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "ax1.set_title('epoch vs loss')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(list_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH3tnHguXKjM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 학습된 모델을 model 변수에 할당한 후 저장\n",
    "model_name = 'swin'\n",
    "torch.save(model, f'./{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxzEpULaXKjM"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "model = torch.load(\"./swin.pth\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU_-26g9f7PJ"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg5z12bIcg8o"
   },
   "source": [
    "# 3. 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59AT3w7pcMuT"
   },
   "source": [
    "**성능 평가를 위해 valid set에 대해서 결과 확인**\n",
    "\n",
    "**[해야될 것] 1)~3) 까지 결과 공유**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iRvWsbJdH9e"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images, masks in tqdm(valid_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJdtQIL8WnKq"
   },
   "source": [
    "### 1) true_mask vs pred_mask 이미지 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du7RN33sdKot"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        if i == 0:\n",
    "            img = cv2.imread(display_list[i])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.uint8).copy()\n",
    "        if i == 1:\n",
    "            img = cv2.imread(display_list[i], 0)\n",
    "        if i == 2:\n",
    "            img = rle_decode(display_list[i], shape = (224, 224)) # shape 설정\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "idx = 6\n",
    "\n",
    "valid_img_path_idx = valid_img_dir + df_valid['img_path'][idx]\n",
    "valid_mask_path_idx = valid_mask_dir + df_valid['mask_path'][idx]\n",
    "valid_pred_mask_idx = result[idx]\n",
    "\n",
    "display_list = [valid_img_path_idx, valid_mask_path_idx, valid_pred_mask_idx]\n",
    "display(display_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4yZ5ZlOWoU5"
   },
   "source": [
    "### 2) Dice Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJm_zsEVfG8b"
   },
   "outputs": [],
   "source": [
    "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
    "    '''\n",
    "    Calculate Dice Score between two binary masks.\n",
    "    '''\n",
    "    intersection = np.sum(prediction * ground_truth)\n",
    "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
    "\n",
    "\n",
    "def calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224)) -> List[float]:\n",
    "    '''\n",
    "    Calculate Dice scores for a dataset.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Keep only the rows in the prediction dataframe that have matching img_ids in the ground truth dataframe\n",
    "    prediction_df = prediction_df[prediction_df.iloc[:, 0].isin(ground_truth_df.iloc[:, 0])]\n",
    "    prediction_df.index = range(prediction_df.shape[0])\n",
    "\n",
    "\n",
    "    # Extract the mask_rle columns\n",
    "    pred_mask_rle = prediction_df.iloc[:, 1]\n",
    "    gt_mask_rle = ground_truth_df.iloc[:, 1]\n",
    "\n",
    "\n",
    "    def calculate_dice(pred_rle, gt_rle):\n",
    "        pred_mask = rle_decode(pred_rle, img_shape)\n",
    "        gt_mask = rle_decode(gt_rle, img_shape)\n",
    "\n",
    "\n",
    "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
    "            return dice_score(pred_mask, gt_mask)\n",
    "        else:\n",
    "            return None  # No valid masks found, return None\n",
    "\n",
    "\n",
    "    dice_scores = Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n",
    "    )\n",
    "\n",
    "\n",
    "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
    "\n",
    "\n",
    "    return np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emWwnB9S9nbo"
   },
   "outputs": [],
   "source": [
    "valid_pred = {'mask_rle': result}\n",
    "prediction_df = pd.DataFrame(data = valid_pred)\n",
    "\n",
    "\n",
    "lst_ground_truth_rle = [rle_encode((validset.__getitem__(i))[1]) for i in range(len(df_valid))]\n",
    "valid_pred = {'mask_rle': lst_ground_truth_rle}\n",
    "ground_truth_df = pd.DataFrame(data = valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vUfwAUDt7J0"
   },
   "outputs": [],
   "source": [
    "# calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PydI60pCMV0F"
   },
   "source": [
    "### 3) Class 별 IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == -1:\n",
    "        return np.zeros(shape)\n",
    "\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, lst_ground_truth_rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def generateConfusionMatrix(ground_truth_mask, pred_mask):\n",
    "    y_true = rle_decode(ground_truth_mask).reshape(-1)\n",
    "    y_pred = rle_decode(pred_mask).reshape(-1)\n",
    "    cMatrix = confusion_matrix(y_true, y_pred)\n",
    "    return cMatrix\n",
    "\n",
    "def generateConfusionMatrixLst(lst_ground_truth_rle, lst_pred_rle):\n",
    "    lst_cMatrix = Parallel(n_jobs=1)(delayed(generateConfusionMatrix)(lst_ground_truth_rle[i], result[i]) for i in range(len(lst_ground_truth_rle)))\n",
    "    return lst_cMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lst_cMatrix = generateConfusionMatrixLst(lst_ground_truth_rle, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbcrKuzDfhPF"
   },
   "outputs": [],
   "source": [
    "def IoU(cMatrix):\n",
    "    Intersection = cMatrix.diagonal()\n",
    "    Union11 = cMatrix.sum(axis = 0)[0] + cMatrix[0][1]\n",
    "    Union22 = cMatrix.sum(axis = 0)[1] + cMatrix[1][0]\n",
    "    Union = np.array([Union11, Union22])\n",
    "    return Intersection / Union\n",
    "\n",
    "# 전체 이미지 IoU 수치에 대하여 평균냄.\n",
    "def totalIoU(lst_cMatrix):\n",
    "    totalIoU = np.array([0, 0], dtype = 'float64')\n",
    "    for cMat in lst_cMatrix:\n",
    "        totalIoU += IoU(cMat)\n",
    "    return totalIoU / len(lst_cMatrix)\n",
    "\n",
    "def eachIoU(lst_cMatrix):\n",
    "    eachIoU = []\n",
    "    for cMat in lst_cMatrix:\n",
    "        eachIoU.append(IoU(cMat))\n",
    "    return eachIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQajFnHwgZXH"
   },
   "outputs": [],
   "source": [
    "# IoU(Lst_cMatrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EegcalN9hhR6"
   },
   "outputs": [],
   "source": [
    "# totalIoU(Lst_cMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drVZ9YBWimB6"
   },
   "outputs": [],
   "source": [
    "totaliou = totalIoU(Lst_cMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8V3_cT_co5S"
   },
   "outputs": [],
   "source": [
    "def printClassScores(totaliou):\n",
    "    label = ['background', 'building']\n",
    "    print('classes          IoU      nIoU')\n",
    "    print('--------------------------------')\n",
    "    for i, iou in enumerate(totaliou):\n",
    "        labelName = label[i]\n",
    "        iouStr = f'{iou:>5.3f}'\n",
    "        niouStr = 'empty'\n",
    "        print('{:<14}: '.format(labelName) + iouStr + '    ' + niouStr)\n",
    "    print('--------------------------------')\n",
    "    print(f'Score Average : {(np.sum(totaliou) / 2):>5.3f}' + '    ' + niouStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8FbFF_djRwC"
   },
   "outputs": [],
   "source": [
    "printClassScores(totaliou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6C2-bTSLtQl"
   },
   "source": [
    "# 4. 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m80Qh2tL9_ZK"
   },
   "outputs": [],
   "source": [
    "class SatelliteDatasetForTest(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = 'data/' + self.data.iloc[idx, 1][1:]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = 'data/' + self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k050sGUtMFo7"
   },
   "outputs": [],
   "source": [
    "test_transform = A.Compose([\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xp2H822dL2JO"
   },
   "outputs": [],
   "source": [
    "test_dataset = SatelliteDatasetForTest(csv_file='./test.csv', transform=test_transform, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jCWDU62Ny2I"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result_test = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result_test.append(-1)\n",
    "            else:\n",
    "                result_test.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86Qao5IfNhfX"
   },
   "source": [
    "# 제출 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zJknfBNOKZ3"
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z39rrxjOOKx1"
   },
   "outputs": [],
   "source": [
    "submit.to_csv(f'./result_{model_name}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
