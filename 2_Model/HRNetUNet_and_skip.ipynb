{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r8v8-i9L6APh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZKIjRgs6MpR",
        "outputId": "38d2869e-eeba-4e3a-f29a-2dd3f460518c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g2Xt-H3I6RUN"
      },
      "outputs": [],
      "source": [
        "def rle_decode(mask_rle, shape):\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape)\n",
        "\n",
        "# RLE 인코딩 함수\n",
        "#이진 이미지 마스크를 런 길이 인코딩(연속된 픽셀의 길이를 나타냄)으로 표현\n",
        "def rle_encode(mask):\n",
        "    pixels = mask.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지 데이터 명암대비 조정\n",
        "def contrast_adjustment(image, alpha, beta):\n",
        "    # 명암 대비 조정 적용\n",
        "    adjusted_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
        "    return adjusted_image"
      ],
      "metadata": {
        "id": "un1aeOf9BguH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지 데이터 노이즈 제거(가우시안 필터링)\n",
        "def image_noise_removal(image, kernel_size=3, sigma=0):\n",
        "    # 가우시안 필터링을 사용하여 이미지 노이즈 제거\n",
        "    filtered_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)\n",
        "\n",
        "    return filtered_image"
      ],
      "metadata": {
        "id": "QHPDuMjRDts1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XHqu21DS6Tpa"
      },
      "outputs": [],
      "source": [
        "class SatelliteDataset(Dataset):\n",
        "    #데이터셋 초기화\n",
        "    def __init__(self, csv_file, transform=None, infer=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        for i in range(len(self.data.iloc[:,1] )):\n",
        "          self.data.iloc[:,1][i]=self.data.iloc[:,1][i].replace(\"./\", \"/\")\n",
        "        self.data.iloc[:,1] = \"./drive/MyDrive/sw_ai\" + self.data.iloc[:,1]\n",
        "        self.transform = transform\n",
        "        self.infer = infer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    #주어진 인덱스에 해당하는 데이터 샘플 반환\n",
        "    def __getitem__(self, idx):\n",
        "        #이미지 파일 로드, RGB형식으로 변환\n",
        "        img_path = self.data.iloc[idx, 1]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        #명암대비 조정 적용 (alpha는 명암조정비율, beta는 명암조정 상수)\n",
        "        image = contrast_adjustment(image, alpha=1.5, beta=10)\n",
        "         # 이미지 노이즈 제거\n",
        "        image = image_noise_removal(image, kernel_size=3, sigma=0)\n",
        "\n",
        "        if self.infer:\n",
        "            if self.transform:\n",
        "                image = self.transform(image=image)['image']\n",
        "            return image\n",
        "\n",
        "        mask_rle = self.data.iloc[idx, 2] #csv파일에서 런-길이 인코딩값 가져옴(문자열)\n",
        "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1])) #런-길이 인코딩된 마스크를 이진 이미지 마스크로 디코딩\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4D-5y-Y6Zq5",
        "outputId": "0f07b760-1e06-4bd1-c8b6-6eb19ff36111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = SatelliteDataset(csv_file='./drive/MyDrive/sw_ai/train.csv', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ij2A0lt16qv_"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HRNet\n",
        "1. 다양한 해상도 특성 캡쳐가능\n",
        "2. 고해상도 특성 유지\n",
        "3. ResnetUNet보다는 무거움\n",
        "4. 고해상도 이미지 분석 작업이라면 더 유리"
      ],
      "metadata": {
        "id": "B9doJu5LMuL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class HRNetUNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super(HRNetUNet, self).__init__()\n",
        "\n",
        "        # HRNet 모델 로드\n",
        "        self.base_model = models.segmentation.hrnet.hrnetv2_hrnet18(pretrained=True)\n",
        "        self.base_layers = list(self.base_model.children())\n",
        "\n",
        "        # Encoder 부분\n",
        "        self.layer0 = nn.Sequential(*self.base_layers[:3])  # size=(N, 64, x.H/2, x.W/2)\n",
        "        self.layer1 = self.base_layers[3]  # size=(N, 64, x.H/4, x.W/4)\n",
        "        self.layer2 = self.base_layers[4]  # size=(N, 18, x.H/4, x.W/4)\n",
        "        self.layer3 = self.base_layers[5]  # size=(N, 36, x.H/8, x.W/8)\n",
        "        self.layer4 = self.base_layers[6]  # size=(N, 72, x.H/16, x.W/16)\n",
        "\n",
        "        # Decoder 부분\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv_up3 = convrelu(36 + 72, 72, 3, 1)  # 36채널과 72채널을 concat하고 72채널의 필터를 적용\n",
        "        self.conv_up2 = convrelu(18 + 72, 36, 3, 1)  # 18채널과 72채널을 concat하고 36채널의 필터를 적용\n",
        "        self.conv_up1 = convrelu(64 + 36, 36, 3, 1)  # 64채널과 36채널을 concat하고 36채널의 필터를 적용\n",
        "        self.conv_up0 = convrelu(64 + 36, 18, 3, 1)  # 64채널과 36채널을 concat하고 18채널의 필터를 적용\n",
        "\n",
        "        self.conv_original_size0 = convrelu(3, 64, 3, 1)  # 입력 이미지의 3채널에 64채널의 필터를 적용\n",
        "        self.conv_original_size1 = convrelu(64, 64, 3, 1)  # 64채널에 64채널의 필터를 적용\n",
        "        self.conv_original_size2 = convrelu(64 + 18, 64, 3, 1)  # 64채널과 18채널을 concat하고 64채널의 필터를 적용\n",
        "\n",
        "        self.conv_last = nn.Conv2d(64, n_class, 1)  # 64채널을 n_class 채널로 변환하는 1x1 컨볼루션 필터\n",
        "\n",
        "    def forward(self, input):\n",
        "        x_original = self.conv_original_size0(input)  # 입력 이미지에 3채널 필터 적용\n",
        "        x_original = self.conv_original_size1(x_original)  # 이전 계층의 출력에 64채널 필터 적용\n",
        "\n",
        "        layer0 = self.layer0(input)  # 입력 이미지에 대한 첫 번째 레이어 적용\n",
        "        layer1 = self.layer1(layer0)  # layer0의 출력에 대한 두 번째 레이어 적용\n",
        "        layer2 = self.layer2(layer1)  # layer1의 출력에 대한 세 번째 레이어 적용\n",
        "        layer3 = self.layer3(layer2)  # layer2의 출력에 대한 네 번째 레이어 적용\n",
        "        layer4 = self.layer4(layer3)  # layer3의 출력에 대한 다섯 번째 레이어 적용\n",
        "\n",
        "        x = self.upsample(layer4)  # layer4의 출력을 2배로 업샘플링\n",
        "        x = torch.cat([x, layer3], dim=1)  # layer4의 출력과 layer3의 출력을 채널 방향으로 concat\n",
        "        x = self.conv_up3(x)  # concat된 텐서에 convrelu 연산 적용\n",
        "\n",
        "        x = self.upsample(x)  # 이전 계층의 출력을 2배로 업샘플링\n",
        "        x = torch.cat([x, layer2], dim=1)  # 이전 계층의 출력과 layer2의 출력을 채널 방향으로 concat\n",
        "        x = self.conv_up2(x)  # concat된 텐서에 convrelu 연산 적용\n",
        "\n",
        "        x = self.upsample(x)  # 이전 계층의 출력을 2배로 업샘플링\n",
        "        x = torch.cat([x, layer1], dim=1)  # 이전 계층의 출력과 layer1의 출력을 채널 방향으로 concat\n",
        "        x = self.conv_up1(x)  # concat된 텐서에 convrelu 연산 적용\n",
        "\n",
        "        x = self.upsample(x)  # 이전 계층의 출력을 2배로 업샘플링\n",
        "        x = torch.cat([x, layer0], dim=1)  # 이전 계층의 출력과 layer0의 출력을 채널 방향으로 concat\n",
        "        x = self.conv_up0(x)  # concat된 텐서에 convrelu 연산 적용\n",
        "\n",
        "        x = self.upsample(x)  # 이전 계층의 출력을 2배로 업샘플링\n",
        "        x = torch.cat([x, x_original], dim=1)  # 이전 계층의 출력과 원본 이미지의 출력을 채널 방향으로 concat\n",
        "        x = self.conv_original_size2(x)  # concat된 텐서에 convrelu 연산 적용\n",
        "\n",
        "        out = self.conv_last(x)  # conv_last를 통해 최종 출력을 얻음\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "EY2_veIENZUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기존 ResNetUNet에 스킵연결 적용"
      ],
      "metadata": {
        "id": "Gn3IiJKBUD8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNetUNet과의 차이점\n",
        "디코더 부분에 스킵연결 레이어를 추가하여 인코더의 특징 맵을 디코더로 직접 전달함,\n",
        " 디코더는 인코더의 저수준 특징과 고수준 특징을 결합하여 더 정확한 분할 결과를 얻을 수 있음.\n",
        ""
      ],
      "metadata": {
        "id": "nQU1Vo2fSyaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetUNetSkip(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super(ResNetUNetSkip, self).__init__()\n",
        "\n",
        "        # ResNet 모델 로드\n",
        "        self.base_model = models.resnet34(pretrained=True)\n",
        "        self.base_layers = list(self.base_model.children())\n",
        "\n",
        "        # 인코더 레이어\n",
        "        self.layer0 = nn.Sequential(*self.base_layers[:3])\n",
        "        self.layer1 = nn.Sequential(*self.base_layers[3:5])\n",
        "        self.layer2 = self.base_layers[5]\n",
        "        self.layer3 = self.base_layers[6]\n",
        "        self.layer4 = self.base_layers[7]\n",
        "\n",
        "        # 디코더 레이어\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.dec4 = convrelu(512, 256, 3, 1)\n",
        "        self.dec3 = convrelu(256 + 256, 256, 3, 1)\n",
        "        self.dec2 = convrelu(256 + 128, 128, 3, 1)\n",
        "        self.dec1 = convrelu(128 + 64, 64, 3, 1)\n",
        "        self.dec0 = convrelu(64, 64, 3, 1)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, n_class, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # 인코더\n",
        "        x0 = self.layer0(input)\n",
        "        x1 = self.layer1(x0)\n",
        "        x2 = self.layer2(x1)\n",
        "        x3 = self.layer3(x2)\n",
        "        x4 = self.layer4(x3)\n",
        "\n",
        "        # 디코더\n",
        "        x = self.upsample(x4)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.dec4(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.dec3(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x1], dim=1)\n",
        "        x = self.dec2(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x0], dim=1)\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        x = self.dec0(x)\n",
        "\n",
        "        out = self.final_conv(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "def convrelu(in_channels, out_channels, kernel_size, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )"
      ],
      "metadata": {
        "id": "ZQy1WGNOSxJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t1Z8Ew1L6saU"
      },
      "outputs": [],
      "source": [
        "def convrelu(in_channels, out_channels, kernel, padding):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
        "    nn.ReLU(inplace=True), #활성화 함수도 변경해볼 것\n",
        "  )\n",
        "\n",
        "\n",
        "class ResNetUNet(nn.Module):\n",
        "  def __init__(self, n_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.base_model = torchvision.models.resnet34(pretrained=True)\n",
        "    self.base_layers = list(self.base_model.children())\n",
        "\n",
        "    self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
        "    self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
        "    self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
        "    self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
        "    self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
        "    self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
        "    self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
        "    self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
        "    self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
        "    self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
        "\n",
        "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
        "    self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
        "    self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
        "    self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
        "\n",
        "    self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
        "    self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
        "    self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
        "\n",
        "    self.conv_last = nn.Conv2d(64, n_class, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x_original = self.conv_original_size0(input)\n",
        "    x_original = self.conv_original_size1(x_original)\n",
        "\n",
        "    layer0 = self.layer0(input)\n",
        "    layer1 = self.layer1(layer0)\n",
        "    layer2 = self.layer2(layer1)\n",
        "    layer3 = self.layer3(layer2)\n",
        "    layer4 = self.layer4(layer3)\n",
        "\n",
        "    layer4 = self.layer4_1x1(layer4)\n",
        "    x = self.upsample(layer4)\n",
        "    layer3 = self.layer3_1x1(layer3)\n",
        "    x = torch.cat([x, layer3], dim=1)\n",
        "    x = self.conv_up3(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    layer2 = self.layer2_1x1(layer2)\n",
        "    x = torch.cat([x, layer2], dim=1)\n",
        "    x = self.conv_up2(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    layer1 = self.layer1_1x1(layer1)\n",
        "    x = torch.cat([x, layer1], dim=1)\n",
        "    x = self.conv_up1(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    layer0 = self.layer0_1x1(layer0)\n",
        "    x = torch.cat([x, layer0], dim=1)\n",
        "    x = self.conv_up0(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    x = torch.cat([x, x_original], dim=1)\n",
        "    x = self.conv_original_size2(x)\n",
        "\n",
        "    out = self.conv_last(x)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZtBvOGu6yWK",
        "outputId": "662f5677-3a9e-40a1-c8da-9b44aae0d876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 91.8MB/s]\n",
            " 38%|███▊      | 169/447 [2:31:43<4:25:04, 57.21s/it]"
          ]
        }
      ],
      "source": [
        "model = ResNetUNet(1).to(device)\n",
        "\n",
        "# loss function과 optimizer 정의\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(20):  # 10 에폭 동안 학습합니다.\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, masks in tqdm(dataloader):\n",
        "        images = images.float().to(device)\n",
        "        masks = masks.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oULOgHTf65hs"
      },
      "outputs": [],
      "source": [
        "test_dataset = SatelliteDataset(csv_file='./drive/MyDrive/sw_ai/test.csv', transform=transform, infer=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77SUtEuN7FE_"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    result = []\n",
        "    for images in tqdm(test_dataloader):\n",
        "        images = images.float().to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
        "        masks = np.squeeze(masks, axis=1)\n",
        "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            mask_rle = rle_encode(masks[i])\n",
        "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
        "                result.append(-1)\n",
        "            else:\n",
        "                result.append(mask_rle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkPcAAV37Mp5"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('./drive/MyDrive/sw_ai/sample_submission.csv')\n",
        "submit['mask_rle'] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsVETu7d7VCG"
      },
      "outputs": [],
      "source": [
        "submit.to_csv('./submit13.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 해상도 계산 함수\n",
        "def measure_resolution(image_path):\n",
        "  image = Imagee.open(image_path)\n",
        "  width, height = image.size\n",
        "  print(width)\n",
        "  print(height)\n",
        "  return width * height"
      ],
      "metadata": {
        "id": "uJeXGGeA2uEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}