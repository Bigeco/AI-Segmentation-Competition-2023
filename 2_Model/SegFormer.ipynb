{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLoUbbWfnnTQ"
   },
   "source": [
    "# 0. Ï¥àÍ∏∞ ÏÑ∏ÌåÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-LjVNJocZ2IR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# PytorchÏóêÏÑú gpuÎ•º ÏÇ¨Ïö©ÌïòÎäî Î∞©Î≤ï.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eSifeRI5NZIR"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import tifffile as tiff\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V0Ucurv9N9xj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vcspYaHHKoOq"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed Í≥†Ï†ï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-KLMtHmhco7"
   },
   "source": [
    "# 1. Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yt_htC4wnOFZ"
   },
   "outputs": [],
   "source": [
    "# RLE ÎîîÏΩîÎî© Ìï®Ïàò\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "# RLE Ïù∏ÏΩîÎî© Ìï®Ïàò\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R2tmmvGUX2so"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "duPMsWN0XKjC"
   },
   "outputs": [],
   "source": [
    "train_img_dir = './pro_data/data_for_training_and_testing/train/images/'\n",
    "train_mask_dir = './pro_data/data_for_training_and_testing/train/masks/'\n",
    "\n",
    "valid_img_dir = './pro_data/data_for_training_and_testing/val/images/'\n",
    "valid_mask_dir = './pro_data/data_for_training_and_testing/val/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_img = sorted(os.listdir(train_img_dir))\n",
    "data_train_mask = sorted(os.listdir(train_mask_dir))\n",
    "\n",
    "data_val_img = sorted(os.listdir(valid_img_dir))\n",
    "data_val_mask = sorted(os.listdir(valid_mask_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'img_path': data_train_img, 'mask_path': data_train_mask})\n",
    "df_valid = pd.DataFrame({'img_path': data_val_img, 'mask_path': data_val_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uNIYghTtZqOO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000patch_01.png</td>\n",
       "      <td>MASK_0000patch_01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001patch_10.png</td>\n",
       "      <td>MASK_0001patch_10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0001patch_11.png</td>\n",
       "      <td>MASK_0001patch_11.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0001patch_12.png</td>\n",
       "      <td>MASK_0001patch_12.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0001patch_13.png</td>\n",
       "      <td>MASK_0001patch_13.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36627</th>\n",
       "      <td>TRAIN_7139patch_22.png</td>\n",
       "      <td>MASK_7139patch_22.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36628</th>\n",
       "      <td>TRAIN_7139patch_23.png</td>\n",
       "      <td>MASK_7139patch_23.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36629</th>\n",
       "      <td>TRAIN_7139patch_31.png</td>\n",
       "      <td>MASK_7139patch_31.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36630</th>\n",
       "      <td>TRAIN_7139patch_32.png</td>\n",
       "      <td>MASK_7139patch_32.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36631</th>\n",
       "      <td>TRAIN_7139patch_33.png</td>\n",
       "      <td>MASK_7139patch_33.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36632 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     img_path              mask_path\n",
       "0      TRAIN_0000patch_01.png  MASK_0000patch_01.png\n",
       "1      TRAIN_0001patch_10.png  MASK_0001patch_10.png\n",
       "2      TRAIN_0001patch_11.png  MASK_0001patch_11.png\n",
       "3      TRAIN_0001patch_12.png  MASK_0001patch_12.png\n",
       "4      TRAIN_0001patch_13.png  MASK_0001patch_13.png\n",
       "...                       ...                    ...\n",
       "36627  TRAIN_7139patch_22.png  MASK_7139patch_22.png\n",
       "36628  TRAIN_7139patch_23.png  MASK_7139patch_23.png\n",
       "36629  TRAIN_7139patch_31.png  MASK_7139patch_31.png\n",
       "36630  TRAIN_7139patch_32.png  MASK_7139patch_32.png\n",
       "36631  TRAIN_7139patch_33.png  MASK_7139patch_33.png\n",
       "\n",
       "[36632 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsScGmliaF8G"
   },
   "source": [
    "### Custom Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mQOJM-cvL_kc"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cv2 import dnn_superres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dK1PxG8UMAxC"
   },
   "outputs": [],
   "source": [
    "# Create an SR object\n",
    "sr = dnn_superres.DnnSuperResImpl_create()\n",
    "\n",
    "# Read the desired model\n",
    "path = \"FSRCNN_x4.pb\"\n",
    "sr.readModel(path)\n",
    "\n",
    "# Set the desired model and scale to get correct pre- and post-processing\n",
    "sr.setModel(\"fsrcnn\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Yo_qzaXXMUzb"
   },
   "outputs": [],
   "source": [
    "tmp2_transform = A.Compose([\n",
    "    A.Resize(896, 896),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Glnq70NyXKjJ"
   },
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, dataset, lst_path, transform=None, infer=False):\n",
    "        self.data = dataset\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "        self.lst_path = lst_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.lst_path[0] + self.data.iloc[idx, 0]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = sr.upsample(image)\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_path = self.lst_path[1] + self.data.iloc[idx, 1]\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = sr.upsample(image)\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OhwHCGNfOCky"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pro_data/data_for_training_and_testing/train/images/TRAIN_0001patch_10.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_dir + df_train.iloc[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwty-NSdbQTB"
   },
   "source": [
    "**[Ìï¥ÏïºÎê† Í≤É] transform Î∞îÍøîÎ≥¥Í∏∞**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eAtvvUOxXKjJ"
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    # A.Resize(224, 224)  # Ï†ÅÏö© ÏïàÌï¥ÎèÑÎê®. Ïù¥ÎØ∏ Ï£ºÏñ¥ÏßÑ Ïù¥ÎØ∏ÏßÄÍ∞Ä 224x224\n",
    "    A.Normalize(),        # Í∏∞Ï°¥\n",
    "    A.HorizontalFlip(),   # ÏÉàÎ°ú Ï∂îÍ∞Ä\n",
    "    A.VerticalFlip(),     # ÏÉàÎ°ú Ï∂îÍ∞Ä\n",
    "    ToTensorV2()          # Í∏∞Ï°¥\n",
    "], is_check_shapes=False)\n",
    "\n",
    "trainset = SatelliteDataset(dataset = df_train, transform=transform, lst_path = [train_img_dir, train_mask_dir])\n",
    "validset = SatelliteDataset(dataset = df_valid, transform=transform, lst_path = [valid_img_dir, valid_mask_dir])\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=2, shuffle=True, num_workers=48)\n",
    "valid_dataloader = DataLoader(validset, batch_size=2, shuffle=False, num_workers=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqOjvz5pXKjI"
   },
   "source": [
    "# 2. Î™®Îç∏ ÌïôÏäµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bhd9eN1bYB9"
   },
   "source": [
    "**[Ìï¥ÏïºÎê† Í≤É] ÏûêÍ∏∞Í∫º Î™®Îç∏Î°ú ÏàòÏ†ïÌïòÎäîÎç∞ Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïù¥Ï¶à 224x224Ïóê ÎßûÏïÑÏïºÎê®.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HDQBk5W5M3eB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in ./.local/lib/python3.8/site-packages (0.6.1)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: timm in ./.local/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: torch>=1.7 in ./.local/lib/python3.8/site-packages (from timm) (2.0.1+cu117)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.8/site-packages (from timm) (0.15.2+cu117)\n",
      "Requirement already satisfied: pyyaml in /compuworks/anaconda3/lib/python3.8/site-packages (from timm) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.8/site-packages (from timm) (0.16.4)\n",
      "Requirement already satisfied: safetensors in ./.local/lib/python3.8/site-packages (from timm) (0.3.1)\n",
      "Requirement already satisfied: filelock in /compuworks/anaconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torch>=1.7->timm) (4.3.0)\n",
      "Requirement already satisfied: sympy in /compuworks/anaconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (1.6.2)\n",
      "Requirement already satisfied: networkx in /compuworks/anaconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (2.5)\n",
      "Requirement already satisfied: jinja2 in /compuworks/anaconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (2.11.2)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.local/lib/python3.8/site-packages (from torch>=1.7->timm) (2.0.0)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (3.26.4)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (2023.6.0)\n",
      "Requirement already satisfied: requests in /compuworks/anaconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (23.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from torchvision->timm) (1.21.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /compuworks/anaconda3/lib/python3.8/site-packages (from torchvision->timm) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /compuworks/anaconda3/lib/python3.8/site-packages (from jinja2->torch>=1.7->timm) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /compuworks/anaconda3/lib/python3.8/site-packages (from networkx->torch>=1.7->timm) (4.4.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /compuworks/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /compuworks/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /compuworks/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /compuworks/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /compuworks/anaconda3/lib/python3.8/site-packages (from sympy->torch>=1.7->timm) (1.1.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XkVkdZmHa6-b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "from math import sqrt\n",
    "\n",
    "class DsConv2d(nn.Module):\n",
    "    '''r\n",
    "    Mix-FFNÏóê Positional encodingÏùÑ ÎåÄÏã†Ìï† 3x3 Depthwise separable convolution\n",
    "    '''\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride=1, bias=True):\n",
    "        super(DsConv2d, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Depthwise Convolution\n",
    "            nn.Conv2d(in_channels=dim_in,\n",
    "                      out_channels=dim_in,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      groups=dim_in,\n",
    "                      bias=bias\n",
    "                      ),\n",
    "            # Pointwise Convolution\n",
    "            nn.Conv2d(in_channels=dim_in,\n",
    "                      out_channels=dim_out,\n",
    "                      kernel_size=1,\n",
    "                      bias=bias\n",
    "                      )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    Efficient Self-Attn blockÍ≥º Mix-FFN block Ï†ÑÏóê ÏÇ¨Ïö©Îê† Layer Normalization\n",
    "    '''\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=1, unbiased=False, keepdim=True).sqrt()  # std = root(variation)\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(self, *, dim, heads, reduction_ratio):\n",
    "        super(EfficientSelfAttention, self).__init__()\n",
    "        self.scale = (dim // heads) ** -0.5  # root(dim head)\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "        self.to_kv = nn.Conv2d(dim, dim*2, reduction_ratio, stride=reduction_ratio, bias=False)  # ESA\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        heads = self.heads\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim=1))  # chunk: dim=1ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú 2Í∞úÎ°ú Ï™ºÍ∞¨\n",
    "        # q,k,vÏùò Ï∞®ÏõêÏùÑ Î™®Îëê ÏïÑÎûòÏôÄ Í∞ôÏù¥ Î≥ÄÍ≤Ω\n",
    "        # batch, (head*channel), w, h -> (batch*head), (w*h), channel\n",
    "        # Ï≤´ stageÏóêÏÑú 16384 -> 256 ÏúºÎ°ú ÌñâÎ†¨Í≥± Ïó∞ÏÇ∞ÏùÑ 64Î∞∞ Í∞êÏÜåÏãúÌÇ¥(Efficient multi head self attention)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h=heads), (q, k, v))\n",
    "\n",
    "        # einsumÏùÑ ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÌñâÎ†¨Í≥±ÏúºÎ°ú ÏÇ¨Ïö© Í∞ÄÎä•, q@kÎ°úÎèÑ Í∞ÄÎä•\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale  # Q*K / root(dim head)\n",
    "        attn = sim.softmax(dim=-1)  # dim=-1 -> channel\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)  # ÏµúÏ¢Ö attention Í≥ÑÏÇ∞\n",
    "        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h=heads, x=h, y=w)  # Ï∞®Ïõê Î≥µÍµ¨\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class MixFeedForward(nn.Module):\n",
    "    def __init__(self, *, dim, expansion_factor):\n",
    "        super(MixFeedForward, self).__init__()\n",
    "        hidden_dim = dim * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1),\n",
    "            DsConv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),  # positional embedding ÎåÄÏ≤¥\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_dim, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MiT(nn.Module):\n",
    "    '''\n",
    "    Mix Transformer encoders\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 channels,\n",
    "                 dims,\n",
    "                 heads,\n",
    "                 ff_expansion,\n",
    "                 reduction_ratio,\n",
    "                 num_layers\n",
    "                 ):\n",
    "        super(MiT, self).__init__()\n",
    "        stage_kernel_stride_pad = ((7, 4, 3), (3, 2, 1), (3, 2, 1), (3, 2, 1))\n",
    "\n",
    "        dims = (channels, *dims)  # *: take off tuple  (3, 32, 64, 160, 256)\n",
    "        dim_pairs = list(zip(dims[:-1], dims[1:]))  # [(3, 32), (32, 64), (64, 160), (160, 256)]\n",
    "\n",
    "        self.stages = nn.ModuleList([])\n",
    "\n",
    "        for (dim_in, dim_out), (kernel, stride, padding), num_layers, ff_expansion, heads, reduction_ratio in zip(dim_pairs, stage_kernel_stride_pad, num_layers, ff_expansion, heads, reduction_ratio):\n",
    "            get_overlap_patches = nn.Unfold(kernel, stride=stride, padding=padding)\n",
    "            overlap_patch_embed = nn.Conv2d(dim_in * kernel ** 2, dim_out, 1)\n",
    "\n",
    "            layers = nn.ModuleList([])\n",
    "\n",
    "            # Layer Norm -> ESA -> Layer Norm -> MFFN\n",
    "            for _ in range(num_layers):\n",
    "                layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim_out, EfficientSelfAttention(dim=dim_out, heads=heads, reduction_ratio=reduction_ratio)),\n",
    "                    PreNorm(dim_out, MixFeedForward(dim=dim_out, expansion_factor=ff_expansion)),\n",
    "                ]))\n",
    "\n",
    "            self.stages.append(nn.ModuleList([\n",
    "               get_overlap_patches,\n",
    "               overlap_patch_embed,\n",
    "               layers\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, return_layer_outputs=False):\n",
    "        h, w = x.shape[-2:]\n",
    "\n",
    "        layer_outputs = []\n",
    "        for (get_overlap_patches, overlap_embed, layers) in self.stages:\n",
    "            x = get_overlap_patches(x)  # (b, c x kernel x kernel, num_patches)\n",
    "\n",
    "            num_patches = x.shape[-1]\n",
    "            ratio = int(sqrt(h * w / num_patches))\n",
    "            x = rearrange(x, 'b c (h w) -> b c h w', h=h // ratio)  # (b, c(cxkernelxkernel), h, w)\n",
    "            x = overlap_embed(x)  # (b c(embed dim) h w)\n",
    "            for (attn, ff) in layers:  # attention, feed forward\n",
    "                x = attn(x) + x  # skip connection\n",
    "                x = ff(x) + x\n",
    "\n",
    "            layer_outputs.append(x)  # multi scale features\n",
    "\n",
    "        return x if not return_layer_outputs else layer_outputs\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "    '''\n",
    "    Default values from Mix Transformer B0\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 dims=(112, 224, 560, 896),   # (32, 64, 160, 256)\n",
    "                 heads=(1, 2, 5, 8),\n",
    "                 ff_expansion=(8, 8, 4, 4),\n",
    "                 reduction_ratio=(8, 4, 2, 1),\n",
    "                 num_layers=(2, 2, 2, 2),\n",
    "                 channels=3,\n",
    "                 decoder_dim=224,\n",
    "                 num_classes=1):\n",
    "        super(SegFormer, self).__init__()\n",
    "        assert all([*map(lambda t: len(t) == 4, (dims, heads, ff_expansion, reduction_ratio, num_layers))]), 'only four stages are allowed, all keyword arguments must be either a single value or a tuple of 4 values'\n",
    "\n",
    "        self.mit = MiT(\n",
    "            channels=channels,\n",
    "            dims=dims,\n",
    "            heads=heads,\n",
    "            ff_expansion=ff_expansion,\n",
    "            reduction_ratio=reduction_ratio,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # ÎÖºÎ¨∏ Î™®Îç∏ Í∑∏Î¶ºÏóêÏÑú MLP Layer\n",
    "        self.to_fused = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv2d(dim, decoder_dim, 1),  # First, multi-level features Fi from the MiT encoder go through an MLP layer to unify the channel dimension.\n",
    "            nn.Upsample(scale_factor=2**i)  # second step, features are up-sampled to 1/4th and concatenated together.\n",
    "        ) for i, dim in enumerate(dims)])\n",
    "\n",
    "        self.to_segmentation = nn.Sequential(\n",
    "            nn.Conv2d(4 * decoder_dim, decoder_dim, 1),  # Third, a MLP layer is adopted to fuse the concatenated features\n",
    "            nn.Conv2d(decoder_dim, num_classes, 1),  # Finally, another MLP layer takes the fused feature to predict the segmentation mask M with a H4 √ó W4 √ó Ncls resolution\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_outputs = self.mit(x, return_layer_outputs=True)\n",
    "\n",
    "        fused = [to_fused(output) for output, to_fused in zip(layer_outputs, self.to_fused)]\n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        return self.to_segmentation(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n",
    "\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    color\n",
    "    Label(  'unlabeled'            ,  0 ,  (  0,  0,  0) ),\n",
    "    Label(  'building'             ,  1 ,  (255,255,255) ),\n",
    "]\n",
    "\n",
    "\n",
    "id2label = { label.id      : label.name     for label           in labels           }\n",
    "label2id = { label         : label_id       for label_id, label in id2label.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.0.proj.weight', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.3.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# define model\n",
    "# model = torch.load(\"./segformer.pth\", map_location=device)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
    "                                                         num_labels=2, \n",
    "                                                         id2label=id2label, \n",
    "                                                         label2id=label2id,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0ntq4eccChO"
   },
   "source": [
    "**[ÏÑ†ÌÉù] lossÏôÄ optimizer Ìï®Ïàò Î∞îÍøîÎ≥¥Í∏∞**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-7fa4e6e26caa>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"mean_iou\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/18316 [00:16<83:40:52, 16.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7988794445991516\n",
      "Mean_iou: 0.39424364123159306\n",
      "Mean accuracy: 0.7884872824631861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 1001/18316 [14:19<4:04:56,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.16213186085224152\n",
      "Mean_iou: 0.26446549039958483\n",
      "Mean accuracy: 0.5289309807991697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 2001/18316 [30:32<3:51:08,  1.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1371101289987564\n",
      "Mean_iou: 0.31807402084081926\n",
      "Mean accuracy: 0.6361480416816385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 2038/18316 [31:03<3:50:21,  1.18it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx, (images, masks) in enumerate(tqdm(train_dataloader)):\n",
    "        # get the inputs\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.long().to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=images, labels=masks)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate\n",
    "        with torch.no_grad():\n",
    "            upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            predicted = upsampled_logits.argmax(dim=1)\n",
    "          \n",
    "            # note that the metric expects predictions + labels as numpy arrays\n",
    "            metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=masks.detach().cpu().numpy())\n",
    "\n",
    "        # let's print loss and metrics every 100 batches\n",
    "        if idx % 1000 == 0:\n",
    "            metrics = metric._compute(num_labels=len(id2label), \n",
    "                                      ignore_index=0,\n",
    "                                      reduce_labels=False, # we've already reduced the labels before)\n",
    "                                      predictions=predicted.detach().cpu().numpy(),\n",
    "                                      references=masks.detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0pLEBdrXKjL"
   },
   "outputs": [],
   "source": [
    "# model Ï¥àÍ∏∞Ìôî\n",
    "# model = torch.load(\"./segformer.pth\", map_location=device)\n",
    "# model = SegFormer().to(device)\n",
    "# model = nn.DataParallel(model, device_ids = [0, 1])\n",
    "# model.to(device)\n",
    "\n",
    "# # loss functionÍ≥º optimizer Ï†ïÏùò\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fia1b9fLXKjM"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# # ====== Loss Fluctuation ====== #\n",
    "# ax1 = fig.add_subplot(1, 2, 1)\n",
    "# ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
    "# ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
    "# ax1.set_xlabel('epoch')\n",
    "# ax1.set_ylabel('loss')\n",
    "# ax1.grid()\n",
    "# ax1.legend()\n",
    "# ax1.set_title('epoch vs loss')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DH3tnHguXKjM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# ÌïôÏäµÎêú Î™®Îç∏ÏùÑ model Î≥ÄÏàòÏóê Ìï†ÎãπÌïú ÌõÑ Ï†ÄÏû•\n",
    "model_name = 'segformer'\n",
    "torch.save(model, f'./{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxzEpULaXKjM"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# model = torch.load(\"./segformer.pth\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU_-26g9f7PJ"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg5z12bIcg8o"
   },
   "source": [
    "# 3. ÏÑ±Îä• ÌèâÍ∞Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59AT3w7pcMuT"
   },
   "source": [
    "**ÏÑ±Îä• ÌèâÍ∞ÄÎ•º ÏúÑÌï¥ valid setÏóê ÎåÄÌï¥ÏÑú Í≤∞Í≥º ÌôïÏù∏**\n",
    "\n",
    "**[Ìï¥ÏïºÎê† Í≤É] 1)~3) ÍπåÏßÄ Í≤∞Í≥º Í≥µÏú†**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 224, 224])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4iRvWsbJdH9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6106 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA__nll_loss2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-71f0d2541822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#         outputs = model(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/segformer/modeling_segformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_loss_ignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsampled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0mvalid_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_loss_ignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA__nll_loss2d_forward)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images, masks in tqdm(valid_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "#         outputs = model(images)\n",
    "        outputs = model(pixel_values=images, labels=masks)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        masks = torch.sigmoid(logits).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # ÏòàÏ∏°Îêú Í±¥Î¨º ÌîΩÏÖÄÏù¥ ÏïÑÏòà ÏóÜÎäî Í≤ΩÏö∞ -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJdtQIL8WnKq"
   },
   "source": [
    "### 1) true_mask vs pred_mask Ïù¥ÎØ∏ÏßÄ ÎπÑÍµê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du7RN33sdKot"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        if i == 0:\n",
    "            img = cv2.imread(display_list[i])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.uint8).copy()\n",
    "        if i == 1:\n",
    "            img = cv2.imread(display_list[i], 0)\n",
    "        if i == 2:\n",
    "            img = rle_decode(display_list[i], shape = (224, 224)) # shape ÏÑ§Ï†ï\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "idx = 3\n",
    "\n",
    "valid_img_path_idx = valid_img_dir + df_valid['img_path'][idx]\n",
    "valid_mask_path_idx = valid_mask_dir + df_valid['mask_path'][idx]\n",
    "valid_pred_mask_idx = result[idx]\n",
    "\n",
    "display_list = [valid_img_path_idx, valid_mask_path_idx, valid_pred_mask_idx]\n",
    "display(display_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4yZ5ZlOWoU5"
   },
   "source": [
    "### 2) Dice Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwIN43JDKSpk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == -1:\n",
    "        return np.zeros(shape)\n",
    "\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJm_zsEVfG8b"
   },
   "outputs": [],
   "source": [
    "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
    "    '''\n",
    "    Calculate Dice Score between two binary masks.\n",
    "    '''\n",
    "    intersection = np.sum(prediction * ground_truth)\n",
    "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
    "\n",
    "\n",
    "def calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224)) -> List[float]:\n",
    "    '''\n",
    "    Calculate Dice scores for a dataset.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Keep only the rows in the prediction dataframe that have matching img_ids in the ground truth dataframe\n",
    "    prediction_df = prediction_df[prediction_df.iloc[:, 0].isin(ground_truth_df.iloc[:, 0])]\n",
    "    prediction_df.index = range(prediction_df.shape[0])\n",
    "\n",
    "\n",
    "    # Extract the mask_rle columns\n",
    "    pred_mask_rle = prediction_df.iloc[:, 1]\n",
    "    gt_mask_rle = ground_truth_df.iloc[:, 1]\n",
    "\n",
    "\n",
    "    def calculate_dice(pred_rle, gt_rle):\n",
    "        pred_mask = rle_decode(pred_rle, img_shape)\n",
    "        gt_mask = rle_decode(gt_rle, img_shape)\n",
    "\n",
    "\n",
    "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
    "            return dice_score(pred_mask, gt_mask)\n",
    "        else:\n",
    "            return None  # No valid masks found, return None\n",
    "\n",
    "\n",
    "    dice_scores = Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n",
    "    )\n",
    "\n",
    "\n",
    "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
    "\n",
    "\n",
    "    return np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emWwnB9S9nbo"
   },
   "outputs": [],
   "source": [
    "valid_pred = {'mask_rle': result}\n",
    "prediction_df = pd.DataFrame(data = valid_pred)\n",
    "\n",
    "\n",
    "lst_ground_truth_rle = [rle_encode((validset.__getitem__(i))[1]) for i in range(len(df_valid))]\n",
    "valid_pred = {'mask_rle': lst_ground_truth_rle}\n",
    "ground_truth_df = pd.DataFrame(data = valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vUfwAUDt7J0"
   },
   "outputs": [],
   "source": [
    "calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PydI60pCMV0F"
   },
   "source": [
    "### 3) Class Î≥Ñ IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr9o8gw7niol"
   },
   "outputs": [],
   "source": [
    "predNoBuildingIdx = list(filter(lambda x: result[x] == -1, range(len(result))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWNmfBJTaGX_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def generateConfusionMatrix(ground_truth_mask, pred_mask):\n",
    "    y_true = sum(rle_decode(ground_truth_mask).tolist(), [])\n",
    "    y_pred = sum(rle_decode(pred_mask).tolist(), [])\n",
    "    cMatrix = confusion_matrix(y_true, y_pred)\n",
    "    return cMatrix\n",
    "\n",
    "def generateConfusionMatrixLst(lst_ground_truth_rle, lst_pred_rle):\n",
    "    lst_cMatrix = Parallel(n_jobs=1)(delayed(generateConfusionMatrix)(lst_ground_truth_rle[i], result[i]) for i in range(len(lst_ground_truth_rle)))\n",
    "    return lst_cMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UwanclfbovH"
   },
   "outputs": [],
   "source": [
    "Lst_cMatrix = generateConfusionMatrixLst(lst_ground_truth_rle, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqQtVfvKeMmJ"
   },
   "outputs": [],
   "source": [
    "# Lst_cMatrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbcrKuzDfhPF"
   },
   "outputs": [],
   "source": [
    "def IoU(cMatrix):\n",
    "    Intersection = cMatrix.diagonal()\n",
    "    Union11 = cMatrix.sum(axis = 0)[0] + cMatrix[0][1]\n",
    "    Union22 = cMatrix.sum(axis = 0)[1] + cMatrix[1][0]\n",
    "    Union = np.array([Union11, Union22])\n",
    "    return Intersection / Union\n",
    "\n",
    "# Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄ IoU ÏàòÏπòÏóê ÎåÄÌïòÏó¨ ÌèâÍ∑†ÎÉÑ.\n",
    "def totalIoU(lst_cMatrix):\n",
    "    totalIoU = np.array([0, 0], dtype = 'float64')\n",
    "    for cMat in lst_cMatrix:\n",
    "        totalIoU += IoU(cMat)\n",
    "    return totalIoU / len(lst_cMatrix)\n",
    "\n",
    "def eachIoU(lst_cMatrix):\n",
    "    eachIoU = []\n",
    "    for cMat in lst_cMatrix:\n",
    "        eachIoU.append(IoU(cMat))\n",
    "    return eachIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQajFnHwgZXH"
   },
   "outputs": [],
   "source": [
    "# IoU(Lst_cMatrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EegcalN9hhR6"
   },
   "outputs": [],
   "source": [
    "# totalIoU(Lst_cMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drVZ9YBWimB6"
   },
   "outputs": [],
   "source": [
    "totaliou = totalIoU(Lst_cMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8V3_cT_co5S"
   },
   "outputs": [],
   "source": [
    "def printClassScores(totaliou):\n",
    "    label = ['background', 'building']\n",
    "    print('classes          IoU      nIoU')\n",
    "    print('--------------------------------')\n",
    "    for i, iou in enumerate(totaliou):\n",
    "        labelName = label[i]\n",
    "        iouStr = f'{iou:>5.3f}'\n",
    "        niouStr = 'empty'\n",
    "        print('{:<14}: '.format(labelName) + iouStr + '    ' + niouStr)\n",
    "    print('--------------------------------')\n",
    "    print(f'Score Average : {(np.sum(totaliou) / 2):>5.3f}' + '    ' + niouStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8FbFF_djRwC"
   },
   "outputs": [],
   "source": [
    "printClassScores(totaliou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6C2-bTSLtQl"
   },
   "source": [
    "# 4. ÏòàÏ∏°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m80Qh2tL9_ZK"
   },
   "outputs": [],
   "source": [
    "class SatelliteDatasetForTest(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k050sGUtMFo7"
   },
   "outputs": [],
   "source": [
    "test_transform = A.Compose([\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xp2H822dL2JO"
   },
   "outputs": [],
   "source": [
    "test_dataset = SatelliteDatasetForTest(csv_file='./test.csv', transform=test_transform, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jCWDU62Ny2I"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result_test = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # ÏòàÏ∏°Îêú Í±¥Î¨º ÌîΩÏÖÄÏù¥ ÏïÑÏòà ÏóÜÎäî Í≤ΩÏö∞ -1\n",
    "                result_test.append(-1)\n",
    "            else:\n",
    "                result_test.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86Qao5IfNhfX"
   },
   "source": [
    "# Ï†úÏ∂ú ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zJknfBNOKZ3"
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z39rrxjOOKx1"
   },
   "outputs": [],
   "source": [
    "submit.to_csv(f'./{model_name}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
