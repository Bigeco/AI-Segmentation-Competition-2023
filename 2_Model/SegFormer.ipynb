{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLoUbbWfnnTQ"
   },
   "source": [
    "# 0. 초기 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-LjVNJocZ2IR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Pytorch에서 gpu를 사용하는 방법.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eSifeRI5NZIR"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import tifffile as tiff\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V0Ucurv9N9xj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vcspYaHHKoOq"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-KLMtHmhco7"
   },
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yt_htC4wnOFZ"
   },
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R2tmmvGUX2so"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "duPMsWN0XKjC"
   },
   "outputs": [],
   "source": [
    "train_img_dir = './pro_data/data_for_training_and_testing/train/images/'\n",
    "train_mask_dir = './pro_data/data_for_training_and_testing/train/masks/'\n",
    "\n",
    "valid_img_dir = './pro_data/data_for_training_and_testing/val/images/'\n",
    "valid_mask_dir = './pro_data/data_for_training_and_testing/val/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_img = sorted(os.listdir(train_img_dir))\n",
    "data_train_mask = sorted(os.listdir(train_mask_dir))\n",
    "\n",
    "data_val_img = sorted(os.listdir(valid_img_dir))\n",
    "data_val_mask = sorted(os.listdir(valid_mask_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'img_path': data_train_img, 'mask_path': data_train_mask})\n",
    "df_valid = pd.DataFrame({'img_path': data_val_img, 'mask_path': data_val_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uNIYghTtZqOO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000patch_01.png</td>\n",
       "      <td>MASK_0000patch_01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001patch_10.png</td>\n",
       "      <td>MASK_0001patch_10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0001patch_11.png</td>\n",
       "      <td>MASK_0001patch_11.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0001patch_12.png</td>\n",
       "      <td>MASK_0001patch_12.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0001patch_13.png</td>\n",
       "      <td>MASK_0001patch_13.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36627</th>\n",
       "      <td>TRAIN_7139patch_22.png</td>\n",
       "      <td>MASK_7139patch_22.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36628</th>\n",
       "      <td>TRAIN_7139patch_23.png</td>\n",
       "      <td>MASK_7139patch_23.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36629</th>\n",
       "      <td>TRAIN_7139patch_31.png</td>\n",
       "      <td>MASK_7139patch_31.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36630</th>\n",
       "      <td>TRAIN_7139patch_32.png</td>\n",
       "      <td>MASK_7139patch_32.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36631</th>\n",
       "      <td>TRAIN_7139patch_33.png</td>\n",
       "      <td>MASK_7139patch_33.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36632 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     img_path              mask_path\n",
       "0      TRAIN_0000patch_01.png  MASK_0000patch_01.png\n",
       "1      TRAIN_0001patch_10.png  MASK_0001patch_10.png\n",
       "2      TRAIN_0001patch_11.png  MASK_0001patch_11.png\n",
       "3      TRAIN_0001patch_12.png  MASK_0001patch_12.png\n",
       "4      TRAIN_0001patch_13.png  MASK_0001patch_13.png\n",
       "...                       ...                    ...\n",
       "36627  TRAIN_7139patch_22.png  MASK_7139patch_22.png\n",
       "36628  TRAIN_7139patch_23.png  MASK_7139patch_23.png\n",
       "36629  TRAIN_7139patch_31.png  MASK_7139patch_31.png\n",
       "36630  TRAIN_7139patch_32.png  MASK_7139patch_32.png\n",
       "36631  TRAIN_7139patch_33.png  MASK_7139patch_33.png\n",
       "\n",
       "[36632 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsScGmliaF8G"
   },
   "source": [
    "### Custom Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mQOJM-cvL_kc"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cv2 import dnn_superres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dK1PxG8UMAxC"
   },
   "outputs": [],
   "source": [
    "# Create an SR object\n",
    "sr = dnn_superres.DnnSuperResImpl_create()\n",
    "\n",
    "# Read the desired model\n",
    "path = \"FSRCNN_x4.pb\"\n",
    "sr.readModel(path)\n",
    "\n",
    "# Set the desired model and scale to get correct pre- and post-processing\n",
    "sr.setModel(\"fsrcnn\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Yo_qzaXXMUzb"
   },
   "outputs": [],
   "source": [
    "tmp2_transform = A.Compose([\n",
    "    A.Resize(896, 896),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Glnq70NyXKjJ"
   },
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, dataset, lst_path, transform=None, infer=False):\n",
    "        self.data = dataset\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "        self.lst_path = lst_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.lst_path[0] + self.data.iloc[idx, 0]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = sr.upsample(image)\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_path = self.lst_path[1] + self.data.iloc[idx, 1]\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = sr.upsample(image)\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OhwHCGNfOCky"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pro_data/data_for_training_and_testing/train/images/TRAIN_0001patch_10.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_dir + df_train.iloc[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwty-NSdbQTB"
   },
   "source": [
    "**[해야될 것] transform 바꿔보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eAtvvUOxXKjJ"
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    # A.Resize(224, 224)  # 적용 안해도됨. 이미 주어진 이미지가 224x224\n",
    "    A.Normalize(),        # 기존\n",
    "    A.HorizontalFlip(),   # 새로 추가\n",
    "    A.VerticalFlip(),     # 새로 추가\n",
    "    ToTensorV2()          # 기존\n",
    "], is_check_shapes=False)\n",
    "\n",
    "trainset = SatelliteDataset(dataset = df_train, transform=transform, lst_path = [train_img_dir, train_mask_dir])\n",
    "validset = SatelliteDataset(dataset = df_valid, transform=transform, lst_path = [valid_img_dir, valid_mask_dir])\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=2, shuffle=True, num_workers=48)\n",
    "valid_dataloader = DataLoader(validset, batch_size=2, shuffle=False, num_workers=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqOjvz5pXKjI"
   },
   "source": [
    "# 2. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bhd9eN1bYB9"
   },
   "source": [
    "**[해야될 것] 자기꺼 모델로 수정하는데 이미지 사이즈 224x224에 맞아야됨.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HDQBk5W5M3eB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in ./.local/lib/python3.8/site-packages (0.6.1)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: timm in ./.local/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: torch>=1.7 in ./.local/lib/python3.8/site-packages (from timm) (2.0.1+cu117)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.8/site-packages (from timm) (0.15.2+cu117)\n",
      "Requirement already satisfied: pyyaml in /compuworks/anaconda3/lib/python3.8/site-packages (from timm) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.8/site-packages (from timm) (0.16.4)\n",
      "Requirement already satisfied: safetensors in ./.local/lib/python3.8/site-packages (from timm) (0.3.1)\n",
      "Requirement already satisfied: filelock in /compuworks/anaconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torch>=1.7->timm) (4.3.0)\n",
      "Requirement already satisfied: sympy in /compuworks/anaconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (1.6.2)\n",
      "Requirement already satisfied: networkx in /compuworks/anaconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (2.5)\n",
      "Requirement already satisfied: jinja2 in /compuworks/anaconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (2.11.2)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.local/lib/python3.8/site-packages (from torch>=1.7->timm) (2.0.0)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (3.26.4)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (2023.6.0)\n",
      "Requirement already satisfied: requests in /compuworks/anaconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (23.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from torchvision->timm) (1.21.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /compuworks/anaconda3/lib/python3.8/site-packages (from torchvision->timm) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /compuworks/anaconda3/lib/python3.8/site-packages (from jinja2->torch>=1.7->timm) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /compuworks/anaconda3/lib/python3.8/site-packages (from networkx->torch>=1.7->timm) (4.4.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /compuworks/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /compuworks/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /compuworks/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /compuworks/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /compuworks/anaconda3/lib/python3.8/site-packages (from sympy->torch>=1.7->timm) (1.1.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XkVkdZmHa6-b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "from math import sqrt\n",
    "\n",
    "class DsConv2d(nn.Module):\n",
    "    '''r\n",
    "    Mix-FFN에 Positional encoding을 대신할 3x3 Depthwise separable convolution\n",
    "    '''\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride=1, bias=True):\n",
    "        super(DsConv2d, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Depthwise Convolution\n",
    "            nn.Conv2d(in_channels=dim_in,\n",
    "                      out_channels=dim_in,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      groups=dim_in,\n",
    "                      bias=bias\n",
    "                      ),\n",
    "            # Pointwise Convolution\n",
    "            nn.Conv2d(in_channels=dim_in,\n",
    "                      out_channels=dim_out,\n",
    "                      kernel_size=1,\n",
    "                      bias=bias\n",
    "                      )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    Efficient Self-Attn block과 Mix-FFN block 전에 사용될 Layer Normalization\n",
    "    '''\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=1, unbiased=False, keepdim=True).sqrt()  # std = root(variation)\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(self, *, dim, heads, reduction_ratio):\n",
    "        super(EfficientSelfAttention, self).__init__()\n",
    "        self.scale = (dim // heads) ** -0.5  # root(dim head)\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "        self.to_kv = nn.Conv2d(dim, dim*2, reduction_ratio, stride=reduction_ratio, bias=False)  # ESA\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        heads = self.heads\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim=1))  # chunk: dim=1을 기준으로 2개로 쪼갬\n",
    "        # q,k,v의 차원을 모두 아래와 같이 변경\n",
    "        # batch, (head*channel), w, h -> (batch*head), (w*h), channel\n",
    "        # 첫 stage에서 16384 -> 256 으로 행렬곱 연산을 64배 감소시킴(Efficient multi head self attention)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h=heads), (q, k, v))\n",
    "\n",
    "        # einsum을 아래와 같이 행렬곱으로 사용 가능, q@k로도 가능\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale  # Q*K / root(dim head)\n",
    "        attn = sim.softmax(dim=-1)  # dim=-1 -> channel\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)  # 최종 attention 계산\n",
    "        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h=heads, x=h, y=w)  # 차원 복구\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class MixFeedForward(nn.Module):\n",
    "    def __init__(self, *, dim, expansion_factor):\n",
    "        super(MixFeedForward, self).__init__()\n",
    "        hidden_dim = dim * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1),\n",
    "            DsConv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),  # positional embedding 대체\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_dim, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MiT(nn.Module):\n",
    "    '''\n",
    "    Mix Transformer encoders\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 channels,\n",
    "                 dims,\n",
    "                 heads,\n",
    "                 ff_expansion,\n",
    "                 reduction_ratio,\n",
    "                 num_layers\n",
    "                 ):\n",
    "        super(MiT, self).__init__()\n",
    "        stage_kernel_stride_pad = ((7, 4, 3), (3, 2, 1), (3, 2, 1), (3, 2, 1))\n",
    "\n",
    "        dims = (channels, *dims)  # *: take off tuple  (3, 32, 64, 160, 256)\n",
    "        dim_pairs = list(zip(dims[:-1], dims[1:]))  # [(3, 32), (32, 64), (64, 160), (160, 256)]\n",
    "\n",
    "        self.stages = nn.ModuleList([])\n",
    "\n",
    "        for (dim_in, dim_out), (kernel, stride, padding), num_layers, ff_expansion, heads, reduction_ratio in zip(dim_pairs, stage_kernel_stride_pad, num_layers, ff_expansion, heads, reduction_ratio):\n",
    "            get_overlap_patches = nn.Unfold(kernel, stride=stride, padding=padding)\n",
    "            overlap_patch_embed = nn.Conv2d(dim_in * kernel ** 2, dim_out, 1)\n",
    "\n",
    "            layers = nn.ModuleList([])\n",
    "\n",
    "            # Layer Norm -> ESA -> Layer Norm -> MFFN\n",
    "            for _ in range(num_layers):\n",
    "                layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim_out, EfficientSelfAttention(dim=dim_out, heads=heads, reduction_ratio=reduction_ratio)),\n",
    "                    PreNorm(dim_out, MixFeedForward(dim=dim_out, expansion_factor=ff_expansion)),\n",
    "                ]))\n",
    "\n",
    "            self.stages.append(nn.ModuleList([\n",
    "               get_overlap_patches,\n",
    "               overlap_patch_embed,\n",
    "               layers\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, return_layer_outputs=False):\n",
    "        h, w = x.shape[-2:]\n",
    "\n",
    "        layer_outputs = []\n",
    "        for (get_overlap_patches, overlap_embed, layers) in self.stages:\n",
    "            x = get_overlap_patches(x)  # (b, c x kernel x kernel, num_patches)\n",
    "\n",
    "            num_patches = x.shape[-1]\n",
    "            ratio = int(sqrt(h * w / num_patches))\n",
    "            x = rearrange(x, 'b c (h w) -> b c h w', h=h // ratio)  # (b, c(cxkernelxkernel), h, w)\n",
    "            x = overlap_embed(x)  # (b c(embed dim) h w)\n",
    "            for (attn, ff) in layers:  # attention, feed forward\n",
    "                x = attn(x) + x  # skip connection\n",
    "                x = ff(x) + x\n",
    "\n",
    "            layer_outputs.append(x)  # multi scale features\n",
    "\n",
    "        return x if not return_layer_outputs else layer_outputs\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "    '''\n",
    "    Default values from Mix Transformer B0\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 dims=(112, 224, 560, 896),   # (32, 64, 160, 256)\n",
    "                 heads=(1, 2, 5, 8),\n",
    "                 ff_expansion=(8, 8, 4, 4),\n",
    "                 reduction_ratio=(8, 4, 2, 1),\n",
    "                 num_layers=(2, 2, 2, 2),\n",
    "                 channels=3,\n",
    "                 decoder_dim=224,\n",
    "                 num_classes=1):\n",
    "        super(SegFormer, self).__init__()\n",
    "        assert all([*map(lambda t: len(t) == 4, (dims, heads, ff_expansion, reduction_ratio, num_layers))]), 'only four stages are allowed, all keyword arguments must be either a single value or a tuple of 4 values'\n",
    "\n",
    "        self.mit = MiT(\n",
    "            channels=channels,\n",
    "            dims=dims,\n",
    "            heads=heads,\n",
    "            ff_expansion=ff_expansion,\n",
    "            reduction_ratio=reduction_ratio,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # 논문 모델 그림에서 MLP Layer\n",
    "        self.to_fused = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv2d(dim, decoder_dim, 1),  # First, multi-level features Fi from the MiT encoder go through an MLP layer to unify the channel dimension.\n",
    "            nn.Upsample(scale_factor=2**i)  # second step, features are up-sampled to 1/4th and concatenated together.\n",
    "        ) for i, dim in enumerate(dims)])\n",
    "\n",
    "        self.to_segmentation = nn.Sequential(\n",
    "            nn.Conv2d(4 * decoder_dim, decoder_dim, 1),  # Third, a MLP layer is adopted to fuse the concatenated features\n",
    "            nn.Conv2d(decoder_dim, num_classes, 1),  # Finally, another MLP layer takes the fused feature to predict the segmentation mask M with a H4 × W4 × Ncls resolution\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_outputs = self.mit(x, return_layer_outputs=True)\n",
    "\n",
    "        fused = [to_fused(output) for output, to_fused in zip(layer_outputs, self.to_fused)]\n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        return self.to_segmentation(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n",
    "\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    color\n",
    "    Label(  'unlabeled'            ,  0 ,  (  0,  0,  0) ),\n",
    "    Label(  'building'             ,  1 ,  (255,255,255) ),\n",
    "]\n",
    "\n",
    "\n",
    "id2label = { label.id      : label.name     for label           in labels           }\n",
    "label2id = { label         : label_id       for label_id, label in id2label.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.0.proj.weight', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.3.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# define model\n",
    "# model = torch.load(\"./segformer.pth\", map_location=device)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
    "                                                         num_labels=2, \n",
    "                                                         id2label=id2label, \n",
    "                                                         label2id=label2id,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0ntq4eccChO"
   },
   "source": [
    "**[선택] loss와 optimizer 함수 바꿔보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-7fa4e6e26caa>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"mean_iou\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/18316 [00:16<83:40:52, 16.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7988794445991516\n",
      "Mean_iou: 0.39424364123159306\n",
      "Mean accuracy: 0.7884872824631861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1001/18316 [14:19<4:04:56,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.16213186085224152\n",
      "Mean_iou: 0.26446549039958483\n",
      "Mean accuracy: 0.5289309807991697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2001/18316 [30:32<3:51:08,  1.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1371101289987564\n",
      "Mean_iou: 0.31807402084081926\n",
      "Mean accuracy: 0.6361480416816385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2038/18316 [31:03<3:50:21,  1.18it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx, (images, masks) in enumerate(tqdm(train_dataloader)):\n",
    "        # get the inputs\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.long().to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=images, labels=masks)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate\n",
    "        with torch.no_grad():\n",
    "            upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            predicted = upsampled_logits.argmax(dim=1)\n",
    "          \n",
    "            # note that the metric expects predictions + labels as numpy arrays\n",
    "            metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=masks.detach().cpu().numpy())\n",
    "\n",
    "        # let's print loss and metrics every 100 batches\n",
    "        if idx % 1000 == 0:\n",
    "            metrics = metric._compute(num_labels=len(id2label), \n",
    "                                      ignore_index=0,\n",
    "                                      reduce_labels=False, # we've already reduced the labels before)\n",
    "                                      predictions=predicted.detach().cpu().numpy(),\n",
    "                                      references=masks.detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0pLEBdrXKjL"
   },
   "outputs": [],
   "source": [
    "# model 초기화\n",
    "# model = torch.load(\"./segformer.pth\", map_location=device)\n",
    "# model = SegFormer().to(device)\n",
    "# model = nn.DataParallel(model, device_ids = [0, 1])\n",
    "# model.to(device)\n",
    "\n",
    "# # loss function과 optimizer 정의\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fia1b9fLXKjM"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# # ====== Loss Fluctuation ====== #\n",
    "# ax1 = fig.add_subplot(1, 2, 1)\n",
    "# ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
    "# ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
    "# ax1.set_xlabel('epoch')\n",
    "# ax1.set_ylabel('loss')\n",
    "# ax1.grid()\n",
    "# ax1.legend()\n",
    "# ax1.set_title('epoch vs loss')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DH3tnHguXKjM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 학습된 모델을 model 변수에 할당한 후 저장\n",
    "model_name = 'segformer'\n",
    "torch.save(model, f'./{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxzEpULaXKjM"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# model = torch.load(\"./segformer.pth\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU_-26g9f7PJ"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg5z12bIcg8o"
   },
   "source": [
    "# 3. 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59AT3w7pcMuT"
   },
   "source": [
    "**성능 평가를 위해 valid set에 대해서 결과 확인**\n",
    "\n",
    "**[해야될 것] 1)~3) 까지 결과 공유**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 224, 224])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4iRvWsbJdH9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6106 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA__nll_loss2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-71f0d2541822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#         outputs = model(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/segformer/modeling_segformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_loss_ignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsampled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0mvalid_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_loss_ignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA__nll_loss2d_forward)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images, masks in tqdm(valid_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "#         outputs = model(images)\n",
    "        outputs = model(pixel_values=images, labels=masks)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        masks = torch.sigmoid(logits).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJdtQIL8WnKq"
   },
   "source": [
    "### 1) true_mask vs pred_mask 이미지 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du7RN33sdKot"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        if i == 0:\n",
    "            img = cv2.imread(display_list[i])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.uint8).copy()\n",
    "        if i == 1:\n",
    "            img = cv2.imread(display_list[i], 0)\n",
    "        if i == 2:\n",
    "            img = rle_decode(display_list[i], shape = (224, 224)) # shape 설정\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "idx = 3\n",
    "\n",
    "valid_img_path_idx = valid_img_dir + df_valid['img_path'][idx]\n",
    "valid_mask_path_idx = valid_mask_dir + df_valid['mask_path'][idx]\n",
    "valid_pred_mask_idx = result[idx]\n",
    "\n",
    "display_list = [valid_img_path_idx, valid_mask_path_idx, valid_pred_mask_idx]\n",
    "display(display_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4yZ5ZlOWoU5"
   },
   "source": [
    "### 2) Dice Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwIN43JDKSpk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == -1:\n",
    "        return np.zeros(shape)\n",
    "\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJm_zsEVfG8b"
   },
   "outputs": [],
   "source": [
    "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
    "    '''\n",
    "    Calculate Dice Score between two binary masks.\n",
    "    '''\n",
    "    intersection = np.sum(prediction * ground_truth)\n",
    "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
    "\n",
    "\n",
    "def calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224)) -> List[float]:\n",
    "    '''\n",
    "    Calculate Dice scores for a dataset.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Keep only the rows in the prediction dataframe that have matching img_ids in the ground truth dataframe\n",
    "    prediction_df = prediction_df[prediction_df.iloc[:, 0].isin(ground_truth_df.iloc[:, 0])]\n",
    "    prediction_df.index = range(prediction_df.shape[0])\n",
    "\n",
    "\n",
    "    # Extract the mask_rle columns\n",
    "    pred_mask_rle = prediction_df.iloc[:, 1]\n",
    "    gt_mask_rle = ground_truth_df.iloc[:, 1]\n",
    "\n",
    "\n",
    "    def calculate_dice(pred_rle, gt_rle):\n",
    "        pred_mask = rle_decode(pred_rle, img_shape)\n",
    "        gt_mask = rle_decode(gt_rle, img_shape)\n",
    "\n",
    "\n",
    "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
    "            return dice_score(pred_mask, gt_mask)\n",
    "        else:\n",
    "            return None  # No valid masks found, return None\n",
    "\n",
    "\n",
    "    dice_scores = Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n",
    "    )\n",
    "\n",
    "\n",
    "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
    "\n",
    "\n",
    "    return np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emWwnB9S9nbo"
   },
   "outputs": [],
   "source": [
    "valid_pred = {'mask_rle': result}\n",
    "prediction_df = pd.DataFrame(data = valid_pred)\n",
    "\n",
    "\n",
    "lst_ground_truth_rle = [rle_encode((validset.__getitem__(i))[1]) for i in range(len(df_valid))]\n",
    "valid_pred = {'mask_rle': lst_ground_truth_rle}\n",
    "ground_truth_df = pd.DataFrame(data = valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vUfwAUDt7J0"
   },
   "outputs": [],
   "source": [
    "calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PydI60pCMV0F"
   },
   "source": [
    "### 3) Class 별 IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr9o8gw7niol"
   },
   "outputs": [],
   "source": [
    "predNoBuildingIdx = list(filter(lambda x: result[x] == -1, range(len(result))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWNmfBJTaGX_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def generateConfusionMatrix(ground_truth_mask, pred_mask):\n",
    "    y_true = sum(rle_decode(ground_truth_mask).tolist(), [])\n",
    "    y_pred = sum(rle_decode(pred_mask).tolist(), [])\n",
    "    cMatrix = confusion_matrix(y_true, y_pred)\n",
    "    return cMatrix\n",
    "\n",
    "def generateConfusionMatrixLst(lst_ground_truth_rle, lst_pred_rle):\n",
    "    lst_cMatrix = Parallel(n_jobs=1)(delayed(generateConfusionMatrix)(lst_ground_truth_rle[i], result[i]) for i in range(len(lst_ground_truth_rle)))\n",
    "    return lst_cMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UwanclfbovH"
   },
   "outputs": [],
   "source": [
    "Lst_cMatrix = generateConfusionMatrixLst(lst_ground_truth_rle, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqQtVfvKeMmJ"
   },
   "outputs": [],
   "source": [
    "# Lst_cMatrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbcrKuzDfhPF"
   },
   "outputs": [],
   "source": [
    "def IoU(cMatrix):\n",
    "    Intersection = cMatrix.diagonal()\n",
    "    Union11 = cMatrix.sum(axis = 0)[0] + cMatrix[0][1]\n",
    "    Union22 = cMatrix.sum(axis = 0)[1] + cMatrix[1][0]\n",
    "    Union = np.array([Union11, Union22])\n",
    "    return Intersection / Union\n",
    "\n",
    "# 전체 이미지 IoU 수치에 대하여 평균냄.\n",
    "def totalIoU(lst_cMatrix):\n",
    "    totalIoU = np.array([0, 0], dtype = 'float64')\n",
    "    for cMat in lst_cMatrix:\n",
    "        totalIoU += IoU(cMat)\n",
    "    return totalIoU / len(lst_cMatrix)\n",
    "\n",
    "def eachIoU(lst_cMatrix):\n",
    "    eachIoU = []\n",
    "    for cMat in lst_cMatrix:\n",
    "        eachIoU.append(IoU(cMat))\n",
    "    return eachIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQajFnHwgZXH"
   },
   "outputs": [],
   "source": [
    "# IoU(Lst_cMatrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EegcalN9hhR6"
   },
   "outputs": [],
   "source": [
    "# totalIoU(Lst_cMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drVZ9YBWimB6"
   },
   "outputs": [],
   "source": [
    "totaliou = totalIoU(Lst_cMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8V3_cT_co5S"
   },
   "outputs": [],
   "source": [
    "def printClassScores(totaliou):\n",
    "    label = ['background', 'building']\n",
    "    print('classes          IoU      nIoU')\n",
    "    print('--------------------------------')\n",
    "    for i, iou in enumerate(totaliou):\n",
    "        labelName = label[i]\n",
    "        iouStr = f'{iou:>5.3f}'\n",
    "        niouStr = 'empty'\n",
    "        print('{:<14}: '.format(labelName) + iouStr + '    ' + niouStr)\n",
    "    print('--------------------------------')\n",
    "    print(f'Score Average : {(np.sum(totaliou) / 2):>5.3f}' + '    ' + niouStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8FbFF_djRwC"
   },
   "outputs": [],
   "source": [
    "printClassScores(totaliou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6C2-bTSLtQl"
   },
   "source": [
    "# 4. 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m80Qh2tL9_ZK"
   },
   "outputs": [],
   "source": [
    "class SatelliteDatasetForTest(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k050sGUtMFo7"
   },
   "outputs": [],
   "source": [
    "test_transform = A.Compose([\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xp2H822dL2JO"
   },
   "outputs": [],
   "source": [
    "test_dataset = SatelliteDatasetForTest(csv_file='./test.csv', transform=test_transform, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jCWDU62Ny2I"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result_test = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result_test.append(-1)\n",
    "            else:\n",
    "                result_test.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86Qao5IfNhfX"
   },
   "source": [
    "# 제출 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zJknfBNOKZ3"
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z39rrxjOOKx1"
   },
   "outputs": [],
   "source": [
    "submit.to_csv(f'./{model_name}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
